{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5008/3741198026.py:2: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
      "/tmp/ipykernel_5008/3741198026.py:3: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n"
     ]
    }
   ],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.006760443547121"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer.cross_entropy_loss(probs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5008/1098455523.py:3: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5008/2688850719.py:6: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
      "/tmp/ipykernel_5008/2688850719.py:7: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
      "/tmp/ipykernel_5008/2688850719.py:13: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
      "/tmp/ipykernel_5008/2688850719.py:14: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5008/3211748656.py:6: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
      "/tmp/ipykernel_5008/3211748656.py:7: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
      "/tmp/ipykernel_5008/3211748656.py:8: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  target_index = np.ones(batch_size, dtype=np.int)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.397363\n",
      "Epoch 1, loss: 2.330354\n",
      "Epoch 2, loss: 2.311002\n",
      "Epoch 3, loss: 2.303897\n",
      "Epoch 4, loss: 2.303257\n",
      "Epoch 5, loss: 2.302898\n",
      "Epoch 6, loss: 2.302564\n",
      "Epoch 7, loss: 2.301815\n",
      "Epoch 8, loss: 2.301252\n",
      "Epoch 9, loss: 2.301256\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc5c60d4070>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.127\n",
      "Epoch 0, loss: 2.301800\n",
      "Epoch 1, loss: 2.302558\n",
      "Epoch 2, loss: 2.302208\n",
      "Epoch 3, loss: 2.303309\n",
      "Epoch 4, loss: 2.302126\n",
      "Epoch 5, loss: 2.301524\n",
      "Epoch 6, loss: 2.301150\n",
      "Epoch 7, loss: 2.301359\n",
      "Epoch 8, loss: 2.302305\n",
      "Epoch 9, loss: 2.302205\n",
      "Epoch 10, loss: 2.301595\n",
      "Epoch 11, loss: 2.302039\n",
      "Epoch 12, loss: 2.301247\n",
      "Epoch 13, loss: 2.301836\n",
      "Epoch 14, loss: 2.302396\n",
      "Epoch 15, loss: 2.302299\n",
      "Epoch 16, loss: 2.301161\n",
      "Epoch 17, loss: 2.302058\n",
      "Epoch 18, loss: 2.302180\n",
      "Epoch 19, loss: 2.302339\n",
      "Epoch 20, loss: 2.302694\n",
      "Epoch 21, loss: 2.301872\n",
      "Epoch 22, loss: 2.302261\n",
      "Epoch 23, loss: 2.302209\n",
      "Epoch 24, loss: 2.302215\n",
      "Epoch 25, loss: 2.301591\n",
      "Epoch 26, loss: 2.301535\n",
      "Epoch 27, loss: 2.302557\n",
      "Epoch 28, loss: 2.301689\n",
      "Epoch 29, loss: 2.301815\n",
      "Epoch 30, loss: 2.301483\n",
      "Epoch 31, loss: 2.301553\n",
      "Epoch 32, loss: 2.301640\n",
      "Epoch 33, loss: 2.301991\n",
      "Epoch 34, loss: 2.302104\n",
      "Epoch 35, loss: 2.301741\n",
      "Epoch 36, loss: 2.302652\n",
      "Epoch 37, loss: 2.303017\n",
      "Epoch 38, loss: 2.301521\n",
      "Epoch 39, loss: 2.301504\n",
      "Epoch 40, loss: 2.301565\n",
      "Epoch 41, loss: 2.302361\n",
      "Epoch 42, loss: 2.302223\n",
      "Epoch 43, loss: 2.302385\n",
      "Epoch 44, loss: 2.302532\n",
      "Epoch 45, loss: 2.301950\n",
      "Epoch 46, loss: 2.301686\n",
      "Epoch 47, loss: 2.301881\n",
      "Epoch 48, loss: 2.302291\n",
      "Epoch 49, loss: 2.302069\n",
      "Epoch 50, loss: 2.301971\n",
      "Epoch 51, loss: 2.302533\n",
      "Epoch 52, loss: 2.302043\n",
      "Epoch 53, loss: 2.302003\n",
      "Epoch 54, loss: 2.301729\n",
      "Epoch 55, loss: 2.301272\n",
      "Epoch 56, loss: 2.302220\n",
      "Epoch 57, loss: 2.301765\n",
      "Epoch 58, loss: 2.301959\n",
      "Epoch 59, loss: 2.302229\n",
      "Epoch 60, loss: 2.301372\n",
      "Epoch 61, loss: 2.302540\n",
      "Epoch 62, loss: 2.301929\n",
      "Epoch 63, loss: 2.301502\n",
      "Epoch 64, loss: 2.302201\n",
      "Epoch 65, loss: 2.302674\n",
      "Epoch 66, loss: 2.302191\n",
      "Epoch 67, loss: 2.302989\n",
      "Epoch 68, loss: 2.301388\n",
      "Epoch 69, loss: 2.301621\n",
      "Epoch 70, loss: 2.301533\n",
      "Epoch 71, loss: 2.302804\n",
      "Epoch 72, loss: 2.301514\n",
      "Epoch 73, loss: 2.300962\n",
      "Epoch 74, loss: 2.302366\n",
      "Epoch 75, loss: 2.301895\n",
      "Epoch 76, loss: 2.301768\n",
      "Epoch 77, loss: 2.302649\n",
      "Epoch 78, loss: 2.302394\n",
      "Epoch 79, loss: 2.301415\n",
      "Epoch 80, loss: 2.302148\n",
      "Epoch 81, loss: 2.302263\n",
      "Epoch 82, loss: 2.301172\n",
      "Epoch 83, loss: 2.302803\n",
      "Epoch 84, loss: 2.301892\n",
      "Epoch 85, loss: 2.302788\n",
      "Epoch 86, loss: 2.302016\n",
      "Epoch 87, loss: 2.302450\n",
      "Epoch 88, loss: 2.302002\n",
      "Epoch 89, loss: 2.302363\n",
      "Epoch 90, loss: 2.301257\n",
      "Epoch 91, loss: 2.302553\n",
      "Epoch 92, loss: 2.301951\n",
      "Epoch 93, loss: 2.300910\n",
      "Epoch 94, loss: 2.302318\n",
      "Epoch 95, loss: 2.301563\n",
      "Epoch 96, loss: 2.302494\n",
      "Epoch 97, loss: 2.301579\n",
      "Epoch 98, loss: 2.302468\n",
      "Epoch 99, loss: 2.302352\n",
      "Accuracy after training for 100 epochs:  0.121\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.301406\n",
      "Epoch 1, loss: 2.300645\n",
      "Epoch 2, loss: 2.301930\n",
      "Epoch 3, loss: 2.300194\n",
      "Epoch 4, loss: 2.296883\n",
      "Epoch 5, loss: 2.297291\n",
      "Epoch 6, loss: 2.293188\n",
      "Epoch 7, loss: 2.297095\n",
      "Epoch 8, loss: 2.290985\n",
      "Epoch 9, loss: 2.289429\n",
      "Epoch 10, loss: 2.289858\n",
      "Epoch 11, loss: 2.289787\n",
      "Epoch 12, loss: 2.291270\n",
      "Epoch 13, loss: 2.294006\n",
      "Epoch 14, loss: 2.290483\n",
      "Epoch 15, loss: 2.281833\n",
      "Epoch 16, loss: 2.288653\n",
      "Epoch 17, loss: 2.280844\n",
      "Epoch 18, loss: 2.286164\n",
      "Epoch 19, loss: 2.283487\n",
      "Epoch 20, loss: 2.280167\n",
      "Epoch 21, loss: 2.281047\n",
      "Epoch 22, loss: 2.282983\n",
      "Epoch 23, loss: 2.283910\n",
      "Epoch 24, loss: 2.285083\n",
      "Epoch 25, loss: 2.279782\n",
      "Epoch 26, loss: 2.276024\n",
      "Epoch 27, loss: 2.280392\n",
      "Epoch 28, loss: 2.280157\n",
      "Epoch 29, loss: 2.286447\n",
      "Epoch 30, loss: 2.276390\n",
      "Epoch 31, loss: 2.277262\n",
      "Epoch 32, loss: 2.271290\n",
      "Epoch 33, loss: 2.273131\n",
      "Epoch 34, loss: 2.270356\n",
      "Epoch 35, loss: 2.275901\n",
      "Epoch 36, loss: 2.266546\n",
      "Epoch 37, loss: 2.280699\n",
      "Epoch 38, loss: 2.273933\n",
      "Epoch 39, loss: 2.270175\n",
      "Epoch 40, loss: 2.266119\n",
      "Epoch 41, loss: 2.267603\n",
      "Epoch 42, loss: 2.266981\n",
      "Epoch 43, loss: 2.276082\n",
      "Epoch 44, loss: 2.264551\n",
      "Epoch 45, loss: 2.263212\n",
      "Epoch 46, loss: 2.256343\n",
      "Epoch 47, loss: 2.265754\n",
      "Epoch 48, loss: 2.273327\n",
      "Epoch 49, loss: 2.271195\n",
      "Epoch 50, loss: 2.272021\n",
      "Epoch 51, loss: 2.268014\n",
      "Epoch 52, loss: 2.260603\n",
      "Epoch 53, loss: 2.258334\n",
      "Epoch 54, loss: 2.249324\n",
      "Epoch 55, loss: 2.274822\n",
      "Epoch 56, loss: 2.257045\n",
      "Epoch 57, loss: 2.254233\n",
      "Epoch 58, loss: 2.267611\n",
      "Epoch 59, loss: 2.256048\n",
      "Epoch 60, loss: 2.259698\n",
      "Epoch 61, loss: 2.259733\n",
      "Epoch 62, loss: 2.264856\n",
      "Epoch 63, loss: 2.242543\n",
      "Epoch 64, loss: 2.259687\n",
      "Epoch 65, loss: 2.258583\n",
      "Epoch 66, loss: 2.251748\n",
      "Epoch 67, loss: 2.252709\n",
      "Epoch 68, loss: 2.247482\n",
      "Epoch 69, loss: 2.265438\n",
      "Epoch 70, loss: 2.251024\n",
      "Epoch 71, loss: 2.254790\n",
      "Epoch 72, loss: 2.266002\n",
      "Epoch 73, loss: 2.252680\n",
      "Epoch 74, loss: 2.261131\n",
      "Epoch 75, loss: 2.240917\n",
      "Epoch 76, loss: 2.252267\n",
      "Epoch 77, loss: 2.249231\n",
      "Epoch 78, loss: 2.255310\n",
      "Epoch 79, loss: 2.247037\n",
      "Epoch 80, loss: 2.249575\n",
      "Epoch 81, loss: 2.257609\n",
      "Epoch 82, loss: 2.250038\n",
      "Epoch 83, loss: 2.224896\n",
      "Epoch 84, loss: 2.251718\n",
      "Epoch 85, loss: 2.243551\n",
      "Epoch 86, loss: 2.238967\n",
      "Epoch 87, loss: 2.234535\n",
      "Epoch 88, loss: 2.248531\n",
      "Epoch 89, loss: 2.236178\n",
      "Epoch 90, loss: 2.244513\n",
      "Epoch 91, loss: 2.242432\n",
      "Epoch 92, loss: 2.245248\n",
      "Epoch 93, loss: 2.234546\n",
      "Epoch 94, loss: 2.239910\n",
      "Epoch 95, loss: 2.244802\n",
      "Epoch 96, loss: 2.210454\n",
      "Epoch 97, loss: 2.249350\n",
      "Epoch 98, loss: 2.239758\n",
      "Epoch 99, loss: 2.244082\n",
      "Epoch 100, loss: 2.221691\n",
      "Epoch 101, loss: 2.248739\n",
      "Epoch 102, loss: 2.228661\n",
      "Epoch 103, loss: 2.228063\n",
      "Epoch 104, loss: 2.249757\n",
      "Epoch 105, loss: 2.227843\n",
      "Epoch 106, loss: 2.224473\n",
      "Epoch 107, loss: 2.231831\n",
      "Epoch 108, loss: 2.240563\n",
      "Epoch 109, loss: 2.226900\n",
      "Epoch 110, loss: 2.226218\n",
      "Epoch 111, loss: 2.244918\n",
      "Epoch 112, loss: 2.218916\n",
      "Epoch 113, loss: 2.234159\n",
      "Epoch 114, loss: 2.236765\n",
      "Epoch 115, loss: 2.242336\n",
      "Epoch 116, loss: 2.252394\n",
      "Epoch 117, loss: 2.205453\n",
      "Epoch 118, loss: 2.228494\n",
      "Epoch 119, loss: 2.240253\n",
      "Epoch 120, loss: 2.254768\n",
      "Epoch 121, loss: 2.217585\n",
      "Epoch 122, loss: 2.234765\n",
      "Epoch 123, loss: 2.225219\n",
      "Epoch 124, loss: 2.226004\n",
      "Epoch 125, loss: 2.221061\n",
      "Epoch 126, loss: 2.217852\n",
      "Epoch 127, loss: 2.207327\n",
      "Epoch 128, loss: 2.223436\n",
      "Epoch 129, loss: 2.209185\n",
      "Epoch 130, loss: 2.205913\n",
      "Epoch 131, loss: 2.220293\n",
      "Epoch 132, loss: 2.220553\n",
      "Epoch 133, loss: 2.230407\n",
      "Epoch 134, loss: 2.222130\n",
      "Epoch 135, loss: 2.221370\n",
      "Epoch 136, loss: 2.203137\n",
      "Epoch 137, loss: 2.203642\n",
      "Epoch 138, loss: 2.219371\n",
      "Epoch 139, loss: 2.221810\n",
      "Epoch 140, loss: 2.218164\n",
      "Epoch 141, loss: 2.206595\n",
      "Epoch 142, loss: 2.222649\n",
      "Epoch 143, loss: 2.238167\n",
      "Epoch 144, loss: 2.213332\n",
      "Epoch 145, loss: 2.223988\n",
      "Epoch 146, loss: 2.223068\n",
      "Epoch 147, loss: 2.224065\n",
      "Epoch 148, loss: 2.221519\n",
      "Epoch 149, loss: 2.200332\n",
      "Epoch 150, loss: 2.231181\n",
      "Epoch 151, loss: 2.230520\n",
      "Epoch 152, loss: 2.223845\n",
      "Epoch 153, loss: 2.223682\n",
      "Epoch 154, loss: 2.221008\n",
      "Epoch 155, loss: 2.224618\n",
      "Epoch 156, loss: 2.229913\n",
      "Epoch 157, loss: 2.228165\n",
      "Epoch 158, loss: 2.218339\n",
      "Epoch 159, loss: 2.206259\n",
      "Epoch 160, loss: 2.204489\n",
      "Epoch 161, loss: 2.200599\n",
      "Epoch 162, loss: 2.201935\n",
      "Epoch 163, loss: 2.198468\n",
      "Epoch 164, loss: 2.235315\n",
      "Epoch 165, loss: 2.204200\n",
      "Epoch 166, loss: 2.199713\n",
      "Epoch 167, loss: 2.202555\n",
      "Epoch 168, loss: 2.211138\n",
      "Epoch 169, loss: 2.199797\n",
      "Epoch 170, loss: 2.185770\n",
      "Epoch 171, loss: 2.205293\n",
      "Epoch 172, loss: 2.198532\n",
      "Epoch 173, loss: 2.229105\n",
      "Epoch 174, loss: 2.199349\n",
      "Epoch 175, loss: 2.218395\n",
      "Epoch 176, loss: 2.200593\n",
      "Epoch 177, loss: 2.225071\n",
      "Epoch 178, loss: 2.206651\n",
      "Epoch 179, loss: 2.199700\n",
      "Epoch 180, loss: 2.219222\n",
      "Epoch 181, loss: 2.224384\n",
      "Epoch 182, loss: 2.222611\n",
      "Epoch 183, loss: 2.199459\n",
      "Epoch 184, loss: 2.223747\n",
      "Epoch 185, loss: 2.203679\n",
      "Epoch 186, loss: 2.208729\n",
      "Epoch 187, loss: 2.224303\n",
      "Epoch 188, loss: 2.193616\n",
      "Epoch 189, loss: 2.191248\n",
      "Epoch 190, loss: 2.199769\n",
      "Epoch 191, loss: 2.228104\n",
      "Epoch 192, loss: 2.221316\n",
      "Epoch 193, loss: 2.175200\n",
      "Epoch 194, loss: 2.191057\n",
      "Epoch 195, loss: 2.216325\n",
      "Epoch 196, loss: 2.201197\n",
      "Epoch 197, loss: 2.202789\n",
      "Epoch 198, loss: 2.214714\n",
      "Epoch 199, loss: 2.177576\n",
      "Epoch 0, loss: 2.301735\n",
      "Epoch 1, loss: 2.300631\n",
      "Epoch 2, loss: 2.300800\n",
      "Epoch 3, loss: 2.298724\n",
      "Epoch 4, loss: 2.296473\n",
      "Epoch 5, loss: 2.295688\n",
      "Epoch 6, loss: 2.291718\n",
      "Epoch 7, loss: 2.291420\n",
      "Epoch 8, loss: 2.296152\n",
      "Epoch 9, loss: 2.292431\n",
      "Epoch 10, loss: 2.290487\n",
      "Epoch 11, loss: 2.292988\n",
      "Epoch 12, loss: 2.293110\n",
      "Epoch 13, loss: 2.295674\n",
      "Epoch 14, loss: 2.284950\n",
      "Epoch 15, loss: 2.292662\n",
      "Epoch 16, loss: 2.285459\n",
      "Epoch 17, loss: 2.288208\n",
      "Epoch 18, loss: 2.289303\n",
      "Epoch 19, loss: 2.286568\n",
      "Epoch 20, loss: 2.279520\n",
      "Epoch 21, loss: 2.286209\n",
      "Epoch 22, loss: 2.283697\n",
      "Epoch 23, loss: 2.283699\n",
      "Epoch 24, loss: 2.287409\n",
      "Epoch 25, loss: 2.282701\n",
      "Epoch 26, loss: 2.280520\n",
      "Epoch 27, loss: 2.271675\n",
      "Epoch 28, loss: 2.281183\n",
      "Epoch 29, loss: 2.273227\n",
      "Epoch 30, loss: 2.267387\n",
      "Epoch 31, loss: 2.278824\n",
      "Epoch 32, loss: 2.283330\n",
      "Epoch 33, loss: 2.267435\n",
      "Epoch 34, loss: 2.270530\n",
      "Epoch 35, loss: 2.271754\n",
      "Epoch 36, loss: 2.261433\n",
      "Epoch 37, loss: 2.275842\n",
      "Epoch 38, loss: 2.264085\n",
      "Epoch 39, loss: 2.272768\n",
      "Epoch 40, loss: 2.260092\n",
      "Epoch 41, loss: 2.276123\n",
      "Epoch 42, loss: 2.272774\n",
      "Epoch 43, loss: 2.262581\n",
      "Epoch 44, loss: 2.272295\n",
      "Epoch 45, loss: 2.270626\n",
      "Epoch 46, loss: 2.269500\n",
      "Epoch 47, loss: 2.272172\n",
      "Epoch 48, loss: 2.260123\n",
      "Epoch 49, loss: 2.269038\n",
      "Epoch 50, loss: 2.250911\n",
      "Epoch 51, loss: 2.257419\n",
      "Epoch 52, loss: 2.253854\n",
      "Epoch 53, loss: 2.271108\n",
      "Epoch 54, loss: 2.249012\n",
      "Epoch 55, loss: 2.257046\n",
      "Epoch 56, loss: 2.271037\n",
      "Epoch 57, loss: 2.254823\n",
      "Epoch 58, loss: 2.264234\n",
      "Epoch 59, loss: 2.256530\n",
      "Epoch 60, loss: 2.265178\n",
      "Epoch 61, loss: 2.249248\n",
      "Epoch 62, loss: 2.251332\n",
      "Epoch 63, loss: 2.263879\n",
      "Epoch 64, loss: 2.251961\n",
      "Epoch 65, loss: 2.267829\n",
      "Epoch 66, loss: 2.250101\n",
      "Epoch 67, loss: 2.259753\n",
      "Epoch 68, loss: 2.268357\n",
      "Epoch 69, loss: 2.246049\n",
      "Epoch 70, loss: 2.254459\n",
      "Epoch 71, loss: 2.253825\n",
      "Epoch 72, loss: 2.234906\n",
      "Epoch 73, loss: 2.253465\n",
      "Epoch 74, loss: 2.260838\n",
      "Epoch 75, loss: 2.246476\n",
      "Epoch 76, loss: 2.257912\n",
      "Epoch 77, loss: 2.243323\n",
      "Epoch 78, loss: 2.231006\n",
      "Epoch 79, loss: 2.248677\n",
      "Epoch 80, loss: 2.246533\n",
      "Epoch 81, loss: 2.247616\n",
      "Epoch 82, loss: 2.247604\n",
      "Epoch 83, loss: 2.241386\n",
      "Epoch 84, loss: 2.257233\n",
      "Epoch 85, loss: 2.237890\n",
      "Epoch 86, loss: 2.245533\n",
      "Epoch 87, loss: 2.252812\n",
      "Epoch 88, loss: 2.245231\n",
      "Epoch 89, loss: 2.248712\n",
      "Epoch 90, loss: 2.225527\n",
      "Epoch 91, loss: 2.246837\n",
      "Epoch 92, loss: 2.225823\n",
      "Epoch 93, loss: 2.250501\n",
      "Epoch 94, loss: 2.241522\n",
      "Epoch 95, loss: 2.241488\n",
      "Epoch 96, loss: 2.235932\n",
      "Epoch 97, loss: 2.230116\n",
      "Epoch 98, loss: 2.237242\n",
      "Epoch 99, loss: 2.259864\n",
      "Epoch 100, loss: 2.251262\n",
      "Epoch 101, loss: 2.244797\n",
      "Epoch 102, loss: 2.237132\n",
      "Epoch 103, loss: 2.241769\n",
      "Epoch 104, loss: 2.256696\n",
      "Epoch 105, loss: 2.235411\n",
      "Epoch 106, loss: 2.211931\n",
      "Epoch 107, loss: 2.219092\n",
      "Epoch 108, loss: 2.238311\n",
      "Epoch 109, loss: 2.228967\n",
      "Epoch 110, loss: 2.237705\n",
      "Epoch 111, loss: 2.230655\n",
      "Epoch 112, loss: 2.248091\n",
      "Epoch 113, loss: 2.233591\n",
      "Epoch 114, loss: 2.238257\n",
      "Epoch 115, loss: 2.238884\n",
      "Epoch 116, loss: 2.218576\n",
      "Epoch 117, loss: 2.235894\n",
      "Epoch 118, loss: 2.239583\n",
      "Epoch 119, loss: 2.254862\n",
      "Epoch 120, loss: 2.231742\n",
      "Epoch 121, loss: 2.231301\n",
      "Epoch 122, loss: 2.233645\n",
      "Epoch 123, loss: 2.234351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 124, loss: 2.213354\n",
      "Epoch 125, loss: 2.239420\n",
      "Epoch 126, loss: 2.207396\n",
      "Epoch 127, loss: 2.205455\n",
      "Epoch 128, loss: 2.220146\n",
      "Epoch 129, loss: 2.232380\n",
      "Epoch 130, loss: 2.229632\n",
      "Epoch 131, loss: 2.245666\n",
      "Epoch 132, loss: 2.232168\n",
      "Epoch 133, loss: 2.212070\n",
      "Epoch 134, loss: 2.214035\n",
      "Epoch 135, loss: 2.219591\n",
      "Epoch 136, loss: 2.213689\n",
      "Epoch 137, loss: 2.231066\n",
      "Epoch 138, loss: 2.214535\n",
      "Epoch 139, loss: 2.227186\n",
      "Epoch 140, loss: 2.222834\n",
      "Epoch 141, loss: 2.214677\n",
      "Epoch 142, loss: 2.229833\n",
      "Epoch 143, loss: 2.227874\n",
      "Epoch 144, loss: 2.200180\n",
      "Epoch 145, loss: 2.223438\n",
      "Epoch 146, loss: 2.205595\n",
      "Epoch 147, loss: 2.230089\n",
      "Epoch 148, loss: 2.206876\n",
      "Epoch 149, loss: 2.225306\n",
      "Epoch 150, loss: 2.203973\n",
      "Epoch 151, loss: 2.230894\n",
      "Epoch 152, loss: 2.213655\n",
      "Epoch 153, loss: 2.213585\n",
      "Epoch 154, loss: 2.226699\n",
      "Epoch 155, loss: 2.195337\n",
      "Epoch 156, loss: 2.212665\n",
      "Epoch 157, loss: 2.212575\n",
      "Epoch 158, loss: 2.225728\n",
      "Epoch 159, loss: 2.200590\n",
      "Epoch 160, loss: 2.227460\n",
      "Epoch 161, loss: 2.210871\n",
      "Epoch 162, loss: 2.228208\n",
      "Epoch 163, loss: 2.207817\n",
      "Epoch 164, loss: 2.206944\n",
      "Epoch 165, loss: 2.206767\n",
      "Epoch 166, loss: 2.193389\n",
      "Epoch 167, loss: 2.206895\n",
      "Epoch 168, loss: 2.234716\n",
      "Epoch 169, loss: 2.198802\n",
      "Epoch 170, loss: 2.194543\n",
      "Epoch 171, loss: 2.208498\n",
      "Epoch 172, loss: 2.222874\n",
      "Epoch 173, loss: 2.203206\n",
      "Epoch 174, loss: 2.213898\n",
      "Epoch 175, loss: 2.222994\n",
      "Epoch 176, loss: 2.201701\n",
      "Epoch 177, loss: 2.220233\n",
      "Epoch 178, loss: 2.220941\n",
      "Epoch 179, loss: 2.197661\n",
      "Epoch 180, loss: 2.209420\n",
      "Epoch 181, loss: 2.199146\n",
      "Epoch 182, loss: 2.214395\n",
      "Epoch 183, loss: 2.224947\n",
      "Epoch 184, loss: 2.219461\n",
      "Epoch 185, loss: 2.227085\n",
      "Epoch 186, loss: 2.211137\n",
      "Epoch 187, loss: 2.193881\n",
      "Epoch 188, loss: 2.206112\n",
      "Epoch 189, loss: 2.183144\n",
      "Epoch 190, loss: 2.196448\n",
      "Epoch 191, loss: 2.186080\n",
      "Epoch 192, loss: 2.192052\n",
      "Epoch 193, loss: 2.203634\n",
      "Epoch 194, loss: 2.201390\n",
      "Epoch 195, loss: 2.214965\n",
      "Epoch 196, loss: 2.191912\n",
      "Epoch 197, loss: 2.217274\n",
      "Epoch 198, loss: 2.184457\n",
      "Epoch 199, loss: 2.189368\n",
      "Epoch 0, loss: 2.302129\n",
      "Epoch 1, loss: 2.300423\n",
      "Epoch 2, loss: 2.297667\n",
      "Epoch 3, loss: 2.298296\n",
      "Epoch 4, loss: 2.297824\n",
      "Epoch 5, loss: 2.296578\n",
      "Epoch 6, loss: 2.293818\n",
      "Epoch 7, loss: 2.294969\n",
      "Epoch 8, loss: 2.294032\n",
      "Epoch 9, loss: 2.292382\n",
      "Epoch 10, loss: 2.287597\n",
      "Epoch 11, loss: 2.294209\n",
      "Epoch 12, loss: 2.293366\n",
      "Epoch 13, loss: 2.291295\n",
      "Epoch 14, loss: 2.286367\n",
      "Epoch 15, loss: 2.291076\n",
      "Epoch 16, loss: 2.285752\n",
      "Epoch 17, loss: 2.290092\n",
      "Epoch 18, loss: 2.286509\n",
      "Epoch 19, loss: 2.282915\n",
      "Epoch 20, loss: 2.287176\n",
      "Epoch 21, loss: 2.287994\n",
      "Epoch 22, loss: 2.279103\n",
      "Epoch 23, loss: 2.283140\n",
      "Epoch 24, loss: 2.280689\n",
      "Epoch 25, loss: 2.282042\n",
      "Epoch 26, loss: 2.281763\n",
      "Epoch 27, loss: 2.278217\n",
      "Epoch 28, loss: 2.284176\n",
      "Epoch 29, loss: 2.275853\n",
      "Epoch 30, loss: 2.280278\n",
      "Epoch 31, loss: 2.274019\n",
      "Epoch 32, loss: 2.272897\n",
      "Epoch 33, loss: 2.274112\n",
      "Epoch 34, loss: 2.282380\n",
      "Epoch 35, loss: 2.270541\n",
      "Epoch 36, loss: 2.277493\n",
      "Epoch 37, loss: 2.265591\n",
      "Epoch 38, loss: 2.273610\n",
      "Epoch 39, loss: 2.263384\n",
      "Epoch 40, loss: 2.263825\n",
      "Epoch 41, loss: 2.273294\n",
      "Epoch 42, loss: 2.265441\n",
      "Epoch 43, loss: 2.271572\n",
      "Epoch 44, loss: 2.267402\n",
      "Epoch 45, loss: 2.276080\n",
      "Epoch 46, loss: 2.263682\n",
      "Epoch 47, loss: 2.264505\n",
      "Epoch 48, loss: 2.266758\n",
      "Epoch 49, loss: 2.264160\n",
      "Epoch 50, loss: 2.262614\n",
      "Epoch 51, loss: 2.265243\n",
      "Epoch 52, loss: 2.256829\n",
      "Epoch 53, loss: 2.268528\n",
      "Epoch 54, loss: 2.263165\n",
      "Epoch 55, loss: 2.257292\n",
      "Epoch 56, loss: 2.271366\n",
      "Epoch 57, loss: 2.249133\n",
      "Epoch 58, loss: 2.255806\n",
      "Epoch 59, loss: 2.257141\n",
      "Epoch 60, loss: 2.261399\n",
      "Epoch 61, loss: 2.254205\n",
      "Epoch 62, loss: 2.258710\n",
      "Epoch 63, loss: 2.251830\n",
      "Epoch 64, loss: 2.246886\n",
      "Epoch 65, loss: 2.257862\n",
      "Epoch 66, loss: 2.253254\n",
      "Epoch 67, loss: 2.265297\n",
      "Epoch 68, loss: 2.247196\n",
      "Epoch 69, loss: 2.254780\n",
      "Epoch 70, loss: 2.244639\n",
      "Epoch 71, loss: 2.248510\n",
      "Epoch 72, loss: 2.264739\n",
      "Epoch 73, loss: 2.260717\n",
      "Epoch 74, loss: 2.256524\n",
      "Epoch 75, loss: 2.248260\n",
      "Epoch 76, loss: 2.264449\n",
      "Epoch 77, loss: 2.244954\n",
      "Epoch 78, loss: 2.256513\n",
      "Epoch 79, loss: 2.247579\n",
      "Epoch 80, loss: 2.247926\n",
      "Epoch 81, loss: 2.237210\n",
      "Epoch 82, loss: 2.241995\n",
      "Epoch 83, loss: 2.258771\n",
      "Epoch 84, loss: 2.232269\n",
      "Epoch 85, loss: 2.250791\n",
      "Epoch 86, loss: 2.236052\n",
      "Epoch 87, loss: 2.260371\n",
      "Epoch 88, loss: 2.240492\n",
      "Epoch 89, loss: 2.237552\n",
      "Epoch 90, loss: 2.236867\n",
      "Epoch 91, loss: 2.240260\n",
      "Epoch 92, loss: 2.234258\n",
      "Epoch 93, loss: 2.246003\n",
      "Epoch 94, loss: 2.246058\n",
      "Epoch 95, loss: 2.238976\n",
      "Epoch 96, loss: 2.253234\n",
      "Epoch 97, loss: 2.241119\n",
      "Epoch 98, loss: 2.233402\n",
      "Epoch 99, loss: 2.229989\n",
      "Epoch 100, loss: 2.232675\n",
      "Epoch 101, loss: 2.241485\n",
      "Epoch 102, loss: 2.222915\n",
      "Epoch 103, loss: 2.238419\n",
      "Epoch 104, loss: 2.231413\n",
      "Epoch 105, loss: 2.258204\n",
      "Epoch 106, loss: 2.248248\n",
      "Epoch 107, loss: 2.228920\n",
      "Epoch 108, loss: 2.231651\n",
      "Epoch 109, loss: 2.225883\n",
      "Epoch 110, loss: 2.233264\n",
      "Epoch 111, loss: 2.235821\n",
      "Epoch 112, loss: 2.211092\n",
      "Epoch 113, loss: 2.220765\n",
      "Epoch 114, loss: 2.232346\n",
      "Epoch 115, loss: 2.220712\n",
      "Epoch 116, loss: 2.238493\n",
      "Epoch 117, loss: 2.211011\n",
      "Epoch 118, loss: 2.222705\n",
      "Epoch 119, loss: 2.247158\n",
      "Epoch 120, loss: 2.227730\n",
      "Epoch 121, loss: 2.231869\n",
      "Epoch 122, loss: 2.240492\n",
      "Epoch 123, loss: 2.227846\n",
      "Epoch 124, loss: 2.239288\n",
      "Epoch 125, loss: 2.233642\n",
      "Epoch 126, loss: 2.229731\n",
      "Epoch 127, loss: 2.232483\n",
      "Epoch 128, loss: 2.200338\n",
      "Epoch 129, loss: 2.245741\n",
      "Epoch 130, loss: 2.231848\n",
      "Epoch 131, loss: 2.228731\n",
      "Epoch 132, loss: 2.219027\n",
      "Epoch 133, loss: 2.229266\n",
      "Epoch 134, loss: 2.212374\n",
      "Epoch 135, loss: 2.228607\n",
      "Epoch 136, loss: 2.214200\n",
      "Epoch 137, loss: 2.200201\n",
      "Epoch 138, loss: 2.212400\n",
      "Epoch 139, loss: 2.214711\n",
      "Epoch 140, loss: 2.226099\n",
      "Epoch 141, loss: 2.216612\n",
      "Epoch 142, loss: 2.233578\n",
      "Epoch 143, loss: 2.216056\n",
      "Epoch 144, loss: 2.220381\n",
      "Epoch 145, loss: 2.252339\n",
      "Epoch 146, loss: 2.234932\n",
      "Epoch 147, loss: 2.212430\n",
      "Epoch 148, loss: 2.205642\n",
      "Epoch 149, loss: 2.215859\n",
      "Epoch 150, loss: 2.230767\n",
      "Epoch 151, loss: 2.220905\n",
      "Epoch 152, loss: 2.240320\n",
      "Epoch 153, loss: 2.213532\n",
      "Epoch 154, loss: 2.185191\n",
      "Epoch 155, loss: 2.208406\n",
      "Epoch 156, loss: 2.215571\n",
      "Epoch 157, loss: 2.233666\n",
      "Epoch 158, loss: 2.228945\n",
      "Epoch 159, loss: 2.224019\n",
      "Epoch 160, loss: 2.200211\n",
      "Epoch 161, loss: 2.211523\n",
      "Epoch 162, loss: 2.220928\n",
      "Epoch 163, loss: 2.180413\n",
      "Epoch 164, loss: 2.183887\n",
      "Epoch 165, loss: 2.215811\n",
      "Epoch 166, loss: 2.187771\n",
      "Epoch 167, loss: 2.216310\n",
      "Epoch 168, loss: 2.223218\n",
      "Epoch 169, loss: 2.213807\n",
      "Epoch 170, loss: 2.204597\n",
      "Epoch 171, loss: 2.207799\n",
      "Epoch 172, loss: 2.212272\n",
      "Epoch 173, loss: 2.190164\n",
      "Epoch 174, loss: 2.213684\n",
      "Epoch 175, loss: 2.217286\n",
      "Epoch 176, loss: 2.175573\n",
      "Epoch 177, loss: 2.190128\n",
      "Epoch 178, loss: 2.215714\n",
      "Epoch 179, loss: 2.199233\n",
      "Epoch 180, loss: 2.213456\n",
      "Epoch 181, loss: 2.198477\n",
      "Epoch 182, loss: 2.226415\n",
      "Epoch 183, loss: 2.189646\n",
      "Epoch 184, loss: 2.203420\n",
      "Epoch 185, loss: 2.200878\n",
      "Epoch 186, loss: 2.205884\n",
      "Epoch 187, loss: 2.173906\n",
      "Epoch 188, loss: 2.160994\n",
      "Epoch 189, loss: 2.201256\n",
      "Epoch 190, loss: 2.220697\n",
      "Epoch 191, loss: 2.185583\n",
      "Epoch 192, loss: 2.202771\n",
      "Epoch 193, loss: 2.217678\n",
      "Epoch 194, loss: 2.207147\n",
      "Epoch 195, loss: 2.215324\n",
      "Epoch 196, loss: 2.206569\n",
      "Epoch 197, loss: 2.214488\n",
      "Epoch 198, loss: 2.211831\n",
      "Epoch 199, loss: 2.191940\n",
      "Epoch 0, loss: 2.301768\n",
      "Epoch 1, loss: 2.303202\n",
      "Epoch 2, loss: 2.302646\n",
      "Epoch 3, loss: 2.302404\n",
      "Epoch 4, loss: 2.303433\n",
      "Epoch 5, loss: 2.302129\n",
      "Epoch 6, loss: 2.300904\n",
      "Epoch 7, loss: 2.301225\n",
      "Epoch 8, loss: 2.302707\n",
      "Epoch 9, loss: 2.301979\n",
      "Epoch 10, loss: 2.300716\n",
      "Epoch 11, loss: 2.300897\n",
      "Epoch 12, loss: 2.300401\n",
      "Epoch 13, loss: 2.300507\n",
      "Epoch 14, loss: 2.300763\n",
      "Epoch 15, loss: 2.300794\n",
      "Epoch 16, loss: 2.300655\n",
      "Epoch 17, loss: 2.301184\n",
      "Epoch 18, loss: 2.300415\n",
      "Epoch 19, loss: 2.300183\n",
      "Epoch 20, loss: 2.300324\n",
      "Epoch 21, loss: 2.301015\n",
      "Epoch 22, loss: 2.300541\n",
      "Epoch 23, loss: 2.299770\n",
      "Epoch 24, loss: 2.300258\n",
      "Epoch 25, loss: 2.298641\n",
      "Epoch 26, loss: 2.299767\n",
      "Epoch 27, loss: 2.299556\n",
      "Epoch 28, loss: 2.299493\n",
      "Epoch 29, loss: 2.299726\n",
      "Epoch 30, loss: 2.298489\n",
      "Epoch 31, loss: 2.299773\n",
      "Epoch 32, loss: 2.299561\n",
      "Epoch 33, loss: 2.300018\n",
      "Epoch 34, loss: 2.298884\n",
      "Epoch 35, loss: 2.298823\n",
      "Epoch 36, loss: 2.297857\n",
      "Epoch 37, loss: 2.299124\n",
      "Epoch 38, loss: 2.298756\n",
      "Epoch 39, loss: 2.298262\n",
      "Epoch 40, loss: 2.297630\n",
      "Epoch 41, loss: 2.300057\n",
      "Epoch 42, loss: 2.296780\n",
      "Epoch 43, loss: 2.299416\n",
      "Epoch 44, loss: 2.297972\n",
      "Epoch 45, loss: 2.298798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46, loss: 2.297125\n",
      "Epoch 47, loss: 2.300320\n",
      "Epoch 48, loss: 2.296503\n",
      "Epoch 49, loss: 2.297206\n",
      "Epoch 50, loss: 2.297678\n",
      "Epoch 51, loss: 2.295386\n",
      "Epoch 52, loss: 2.298823\n",
      "Epoch 53, loss: 2.299984\n",
      "Epoch 54, loss: 2.298266\n",
      "Epoch 55, loss: 2.298445\n",
      "Epoch 56, loss: 2.296018\n",
      "Epoch 57, loss: 2.293955\n",
      "Epoch 58, loss: 2.293544\n",
      "Epoch 59, loss: 2.297933\n",
      "Epoch 60, loss: 2.296878\n",
      "Epoch 61, loss: 2.299252\n",
      "Epoch 62, loss: 2.296704\n",
      "Epoch 63, loss: 2.297080\n",
      "Epoch 64, loss: 2.296752\n",
      "Epoch 65, loss: 2.296030\n",
      "Epoch 66, loss: 2.296714\n",
      "Epoch 67, loss: 2.292626\n",
      "Epoch 68, loss: 2.298876\n",
      "Epoch 69, loss: 2.294463\n",
      "Epoch 70, loss: 2.293619\n",
      "Epoch 71, loss: 2.294193\n",
      "Epoch 72, loss: 2.296057\n",
      "Epoch 73, loss: 2.294224\n",
      "Epoch 74, loss: 2.296006\n",
      "Epoch 75, loss: 2.297610\n",
      "Epoch 76, loss: 2.294625\n",
      "Epoch 77, loss: 2.296302\n",
      "Epoch 78, loss: 2.293777\n",
      "Epoch 79, loss: 2.295422\n",
      "Epoch 80, loss: 2.293928\n",
      "Epoch 81, loss: 2.294437\n",
      "Epoch 82, loss: 2.295817\n",
      "Epoch 83, loss: 2.294605\n",
      "Epoch 84, loss: 2.294844\n",
      "Epoch 85, loss: 2.293008\n",
      "Epoch 86, loss: 2.294338\n",
      "Epoch 87, loss: 2.292312\n",
      "Epoch 88, loss: 2.290189\n",
      "Epoch 89, loss: 2.292825\n",
      "Epoch 90, loss: 2.292054\n",
      "Epoch 91, loss: 2.294738\n",
      "Epoch 92, loss: 2.295472\n",
      "Epoch 93, loss: 2.291825\n",
      "Epoch 94, loss: 2.289544\n",
      "Epoch 95, loss: 2.297132\n",
      "Epoch 96, loss: 2.294282\n",
      "Epoch 97, loss: 2.297401\n",
      "Epoch 98, loss: 2.292837\n",
      "Epoch 99, loss: 2.292070\n",
      "Epoch 100, loss: 2.295110\n",
      "Epoch 101, loss: 2.290998\n",
      "Epoch 102, loss: 2.296297\n",
      "Epoch 103, loss: 2.291156\n",
      "Epoch 104, loss: 2.295694\n",
      "Epoch 105, loss: 2.293047\n",
      "Epoch 106, loss: 2.287786\n",
      "Epoch 107, loss: 2.291837\n",
      "Epoch 108, loss: 2.293874\n",
      "Epoch 109, loss: 2.293989\n",
      "Epoch 110, loss: 2.288240\n",
      "Epoch 111, loss: 2.292050\n",
      "Epoch 112, loss: 2.290021\n",
      "Epoch 113, loss: 2.292534\n",
      "Epoch 114, loss: 2.296709\n",
      "Epoch 115, loss: 2.295221\n",
      "Epoch 116, loss: 2.294517\n",
      "Epoch 117, loss: 2.289612\n",
      "Epoch 118, loss: 2.289395\n",
      "Epoch 119, loss: 2.290062\n",
      "Epoch 120, loss: 2.290831\n",
      "Epoch 121, loss: 2.289117\n",
      "Epoch 122, loss: 2.291125\n",
      "Epoch 123, loss: 2.293774\n",
      "Epoch 124, loss: 2.292043\n",
      "Epoch 125, loss: 2.289050\n",
      "Epoch 126, loss: 2.290774\n",
      "Epoch 127, loss: 2.293200\n",
      "Epoch 128, loss: 2.291439\n",
      "Epoch 129, loss: 2.297280\n",
      "Epoch 130, loss: 2.288472\n",
      "Epoch 131, loss: 2.288910\n",
      "Epoch 132, loss: 2.288482\n",
      "Epoch 133, loss: 2.291179\n",
      "Epoch 134, loss: 2.291846\n",
      "Epoch 135, loss: 2.290956\n",
      "Epoch 136, loss: 2.289554\n",
      "Epoch 137, loss: 2.290442\n",
      "Epoch 138, loss: 2.288175\n",
      "Epoch 139, loss: 2.290609\n",
      "Epoch 140, loss: 2.289157\n",
      "Epoch 141, loss: 2.286643\n",
      "Epoch 142, loss: 2.284647\n",
      "Epoch 143, loss: 2.288590\n",
      "Epoch 144, loss: 2.294317\n",
      "Epoch 145, loss: 2.292301\n",
      "Epoch 146, loss: 2.292526\n",
      "Epoch 147, loss: 2.287678\n",
      "Epoch 148, loss: 2.288296\n",
      "Epoch 149, loss: 2.287669\n",
      "Epoch 150, loss: 2.289377\n",
      "Epoch 151, loss: 2.289563\n",
      "Epoch 152, loss: 2.286098\n",
      "Epoch 153, loss: 2.285886\n",
      "Epoch 154, loss: 2.286502\n",
      "Epoch 155, loss: 2.289655\n",
      "Epoch 156, loss: 2.289064\n",
      "Epoch 157, loss: 2.286336\n",
      "Epoch 158, loss: 2.288684\n",
      "Epoch 159, loss: 2.289575\n",
      "Epoch 160, loss: 2.284893\n",
      "Epoch 161, loss: 2.290008\n",
      "Epoch 162, loss: 2.287396\n",
      "Epoch 163, loss: 2.289044\n",
      "Epoch 164, loss: 2.286842\n",
      "Epoch 165, loss: 2.282991\n",
      "Epoch 166, loss: 2.295489\n",
      "Epoch 167, loss: 2.291475\n",
      "Epoch 168, loss: 2.289270\n",
      "Epoch 169, loss: 2.288535\n",
      "Epoch 170, loss: 2.288244\n",
      "Epoch 171, loss: 2.284457\n",
      "Epoch 172, loss: 2.284351\n",
      "Epoch 173, loss: 2.287678\n",
      "Epoch 174, loss: 2.291125\n",
      "Epoch 175, loss: 2.283330\n",
      "Epoch 176, loss: 2.289206\n",
      "Epoch 177, loss: 2.285117\n",
      "Epoch 178, loss: 2.282336\n",
      "Epoch 179, loss: 2.284365\n",
      "Epoch 180, loss: 2.283475\n",
      "Epoch 181, loss: 2.285052\n",
      "Epoch 182, loss: 2.282081\n",
      "Epoch 183, loss: 2.292440\n",
      "Epoch 184, loss: 2.291659\n",
      "Epoch 185, loss: 2.288653\n",
      "Epoch 186, loss: 2.287853\n",
      "Epoch 187, loss: 2.292818\n",
      "Epoch 188, loss: 2.288532\n",
      "Epoch 189, loss: 2.284549\n",
      "Epoch 190, loss: 2.281103\n",
      "Epoch 191, loss: 2.285795\n",
      "Epoch 192, loss: 2.285805\n",
      "Epoch 193, loss: 2.287722\n",
      "Epoch 194, loss: 2.290665\n",
      "Epoch 195, loss: 2.287511\n",
      "Epoch 196, loss: 2.287010\n",
      "Epoch 197, loss: 2.284331\n",
      "Epoch 198, loss: 2.286515\n",
      "Epoch 199, loss: 2.283061\n",
      "Epoch 0, loss: 2.301771\n",
      "Epoch 1, loss: 2.301801\n",
      "Epoch 2, loss: 2.302836\n",
      "Epoch 3, loss: 2.302236\n",
      "Epoch 4, loss: 2.302114\n",
      "Epoch 5, loss: 2.301729\n",
      "Epoch 6, loss: 2.301990\n",
      "Epoch 7, loss: 2.301763\n",
      "Epoch 8, loss: 2.302154\n",
      "Epoch 9, loss: 2.302165\n",
      "Epoch 10, loss: 2.301884\n",
      "Epoch 11, loss: 2.301087\n",
      "Epoch 12, loss: 2.301613\n",
      "Epoch 13, loss: 2.301587\n",
      "Epoch 14, loss: 2.300639\n",
      "Epoch 15, loss: 2.301049\n",
      "Epoch 16, loss: 2.301050\n",
      "Epoch 17, loss: 2.300079\n",
      "Epoch 18, loss: 2.299898\n",
      "Epoch 19, loss: 2.298835\n",
      "Epoch 20, loss: 2.300349\n",
      "Epoch 21, loss: 2.300783\n",
      "Epoch 22, loss: 2.300154\n",
      "Epoch 23, loss: 2.300723\n",
      "Epoch 24, loss: 2.300297\n",
      "Epoch 25, loss: 2.299757\n",
      "Epoch 26, loss: 2.300230\n",
      "Epoch 27, loss: 2.299682\n",
      "Epoch 28, loss: 2.300431\n",
      "Epoch 29, loss: 2.299867\n",
      "Epoch 30, loss: 2.298177\n",
      "Epoch 31, loss: 2.298986\n",
      "Epoch 32, loss: 2.299464\n",
      "Epoch 33, loss: 2.298929\n",
      "Epoch 34, loss: 2.299550\n",
      "Epoch 35, loss: 2.300027\n",
      "Epoch 36, loss: 2.299648\n",
      "Epoch 37, loss: 2.298236\n",
      "Epoch 38, loss: 2.300592\n",
      "Epoch 39, loss: 2.299407\n",
      "Epoch 40, loss: 2.296185\n",
      "Epoch 41, loss: 2.298028\n",
      "Epoch 42, loss: 2.298340\n",
      "Epoch 43, loss: 2.296797\n",
      "Epoch 44, loss: 2.299682\n",
      "Epoch 45, loss: 2.300388\n",
      "Epoch 46, loss: 2.298078\n",
      "Epoch 47, loss: 2.296636\n",
      "Epoch 48, loss: 2.298099\n",
      "Epoch 49, loss: 2.298134\n",
      "Epoch 50, loss: 2.298220\n",
      "Epoch 51, loss: 2.299224\n",
      "Epoch 52, loss: 2.299116\n",
      "Epoch 53, loss: 2.294596\n",
      "Epoch 54, loss: 2.296571\n",
      "Epoch 55, loss: 2.296175\n",
      "Epoch 56, loss: 2.299358\n",
      "Epoch 57, loss: 2.295367\n",
      "Epoch 58, loss: 2.295575\n",
      "Epoch 59, loss: 2.297269\n",
      "Epoch 60, loss: 2.297304\n",
      "Epoch 61, loss: 2.294492\n",
      "Epoch 62, loss: 2.296729\n",
      "Epoch 63, loss: 2.297252\n",
      "Epoch 64, loss: 2.297217\n",
      "Epoch 65, loss: 2.290753\n",
      "Epoch 66, loss: 2.297430\n",
      "Epoch 67, loss: 2.294612\n",
      "Epoch 68, loss: 2.296475\n",
      "Epoch 69, loss: 2.295757\n",
      "Epoch 70, loss: 2.299547\n",
      "Epoch 71, loss: 2.294859\n",
      "Epoch 72, loss: 2.295719\n",
      "Epoch 73, loss: 2.294646\n",
      "Epoch 74, loss: 2.294914\n",
      "Epoch 75, loss: 2.299166\n",
      "Epoch 76, loss: 2.297799\n",
      "Epoch 77, loss: 2.295884\n",
      "Epoch 78, loss: 2.294490\n",
      "Epoch 79, loss: 2.292773\n",
      "Epoch 80, loss: 2.294374\n",
      "Epoch 81, loss: 2.293687\n",
      "Epoch 82, loss: 2.293659\n",
      "Epoch 83, loss: 2.293915\n",
      "Epoch 84, loss: 2.296139\n",
      "Epoch 85, loss: 2.296986\n",
      "Epoch 86, loss: 2.297947\n",
      "Epoch 87, loss: 2.292514\n",
      "Epoch 88, loss: 2.294483\n",
      "Epoch 89, loss: 2.293741\n",
      "Epoch 90, loss: 2.295567\n",
      "Epoch 91, loss: 2.292811\n",
      "Epoch 92, loss: 2.293770\n",
      "Epoch 93, loss: 2.293435\n",
      "Epoch 94, loss: 2.294769\n",
      "Epoch 95, loss: 2.297693\n",
      "Epoch 96, loss: 2.288178\n",
      "Epoch 97, loss: 2.289431\n",
      "Epoch 98, loss: 2.293144\n",
      "Epoch 99, loss: 2.293767\n",
      "Epoch 100, loss: 2.295893\n",
      "Epoch 101, loss: 2.290133\n",
      "Epoch 102, loss: 2.293172\n",
      "Epoch 103, loss: 2.294737\n",
      "Epoch 104, loss: 2.293102\n",
      "Epoch 105, loss: 2.298010\n",
      "Epoch 106, loss: 2.289758\n",
      "Epoch 107, loss: 2.292678\n",
      "Epoch 108, loss: 2.292527\n",
      "Epoch 109, loss: 2.293150\n",
      "Epoch 110, loss: 2.292302\n",
      "Epoch 111, loss: 2.292713\n",
      "Epoch 112, loss: 2.288106\n",
      "Epoch 113, loss: 2.295037\n",
      "Epoch 114, loss: 2.290170\n",
      "Epoch 115, loss: 2.292152\n",
      "Epoch 116, loss: 2.289716\n",
      "Epoch 117, loss: 2.290346\n",
      "Epoch 118, loss: 2.296326\n",
      "Epoch 119, loss: 2.291147\n",
      "Epoch 120, loss: 2.290247\n",
      "Epoch 121, loss: 2.290155\n",
      "Epoch 122, loss: 2.290092\n",
      "Epoch 123, loss: 2.294393\n",
      "Epoch 124, loss: 2.293103\n",
      "Epoch 125, loss: 2.288773\n",
      "Epoch 126, loss: 2.288387\n",
      "Epoch 127, loss: 2.291905\n",
      "Epoch 128, loss: 2.286013\n",
      "Epoch 129, loss: 2.292267\n",
      "Epoch 130, loss: 2.291583\n",
      "Epoch 131, loss: 2.289003\n",
      "Epoch 132, loss: 2.291311\n",
      "Epoch 133, loss: 2.293874\n",
      "Epoch 134, loss: 2.288034\n",
      "Epoch 135, loss: 2.287981\n",
      "Epoch 136, loss: 2.290460\n",
      "Epoch 137, loss: 2.290029\n",
      "Epoch 138, loss: 2.288703\n",
      "Epoch 139, loss: 2.285689\n",
      "Epoch 140, loss: 2.292271\n",
      "Epoch 141, loss: 2.289243\n",
      "Epoch 142, loss: 2.290327\n",
      "Epoch 143, loss: 2.291907\n",
      "Epoch 144, loss: 2.294281\n",
      "Epoch 145, loss: 2.290237\n",
      "Epoch 146, loss: 2.292154\n",
      "Epoch 147, loss: 2.288610\n",
      "Epoch 148, loss: 2.290894\n",
      "Epoch 149, loss: 2.291323\n",
      "Epoch 150, loss: 2.292885\n",
      "Epoch 151, loss: 2.288806\n",
      "Epoch 152, loss: 2.292627\n",
      "Epoch 153, loss: 2.285790\n",
      "Epoch 154, loss: 2.288536\n",
      "Epoch 155, loss: 2.284126\n",
      "Epoch 156, loss: 2.289910\n",
      "Epoch 157, loss: 2.288666\n",
      "Epoch 158, loss: 2.283433\n",
      "Epoch 159, loss: 2.288280\n",
      "Epoch 160, loss: 2.284458\n",
      "Epoch 161, loss: 2.288648\n",
      "Epoch 162, loss: 2.287604\n",
      "Epoch 163, loss: 2.289654\n",
      "Epoch 164, loss: 2.283753\n",
      "Epoch 165, loss: 2.287051\n",
      "Epoch 166, loss: 2.286715\n",
      "Epoch 167, loss: 2.291569\n",
      "Epoch 168, loss: 2.289339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169, loss: 2.287323\n",
      "Epoch 170, loss: 2.285975\n",
      "Epoch 171, loss: 2.282980\n",
      "Epoch 172, loss: 2.291652\n",
      "Epoch 173, loss: 2.279160\n",
      "Epoch 174, loss: 2.287349\n",
      "Epoch 175, loss: 2.286837\n",
      "Epoch 176, loss: 2.289794\n",
      "Epoch 177, loss: 2.284022\n",
      "Epoch 178, loss: 2.287745\n",
      "Epoch 179, loss: 2.282112\n",
      "Epoch 180, loss: 2.287697\n",
      "Epoch 181, loss: 2.285027\n",
      "Epoch 182, loss: 2.284646\n",
      "Epoch 183, loss: 2.287859\n",
      "Epoch 184, loss: 2.284053\n",
      "Epoch 185, loss: 2.286864\n",
      "Epoch 186, loss: 2.286501\n",
      "Epoch 187, loss: 2.285809\n",
      "Epoch 188, loss: 2.291108\n",
      "Epoch 189, loss: 2.281090\n",
      "Epoch 190, loss: 2.281911\n",
      "Epoch 191, loss: 2.285369\n",
      "Epoch 192, loss: 2.282304\n",
      "Epoch 193, loss: 2.284124\n",
      "Epoch 194, loss: 2.281656\n",
      "Epoch 195, loss: 2.286627\n",
      "Epoch 196, loss: 2.291401\n",
      "Epoch 197, loss: 2.285764\n",
      "Epoch 198, loss: 2.289450\n",
      "Epoch 199, loss: 2.289842\n",
      "Epoch 0, loss: 2.302848\n",
      "Epoch 1, loss: 2.302363\n",
      "Epoch 2, loss: 2.302331\n",
      "Epoch 3, loss: 2.301634\n",
      "Epoch 4, loss: 2.301261\n",
      "Epoch 5, loss: 2.302085\n",
      "Epoch 6, loss: 2.302478\n",
      "Epoch 7, loss: 2.300776\n",
      "Epoch 8, loss: 2.300690\n",
      "Epoch 9, loss: 2.301700\n",
      "Epoch 10, loss: 2.302105\n",
      "Epoch 11, loss: 2.301545\n",
      "Epoch 12, loss: 2.301523\n",
      "Epoch 13, loss: 2.301330\n",
      "Epoch 14, loss: 2.302018\n",
      "Epoch 15, loss: 2.300394\n",
      "Epoch 16, loss: 2.300759\n",
      "Epoch 17, loss: 2.300536\n",
      "Epoch 18, loss: 2.301423\n",
      "Epoch 19, loss: 2.299925\n",
      "Epoch 20, loss: 2.299701\n",
      "Epoch 21, loss: 2.299105\n",
      "Epoch 22, loss: 2.301018\n",
      "Epoch 23, loss: 2.300740\n",
      "Epoch 24, loss: 2.299256\n",
      "Epoch 25, loss: 2.299952\n",
      "Epoch 26, loss: 2.299323\n",
      "Epoch 27, loss: 2.299424\n",
      "Epoch 28, loss: 2.300726\n",
      "Epoch 29, loss: 2.298864\n",
      "Epoch 30, loss: 2.300104\n",
      "Epoch 31, loss: 2.299787\n",
      "Epoch 32, loss: 2.299663\n",
      "Epoch 33, loss: 2.298825\n",
      "Epoch 34, loss: 2.299859\n",
      "Epoch 35, loss: 2.298456\n",
      "Epoch 36, loss: 2.301551\n",
      "Epoch 37, loss: 2.299270\n",
      "Epoch 38, loss: 2.298937\n",
      "Epoch 39, loss: 2.298990\n",
      "Epoch 40, loss: 2.298464\n",
      "Epoch 41, loss: 2.298814\n",
      "Epoch 42, loss: 2.298182\n",
      "Epoch 43, loss: 2.297778\n",
      "Epoch 44, loss: 2.298529\n",
      "Epoch 45, loss: 2.296738\n",
      "Epoch 46, loss: 2.300233\n",
      "Epoch 47, loss: 2.297989\n",
      "Epoch 48, loss: 2.299970\n",
      "Epoch 49, loss: 2.297450\n",
      "Epoch 50, loss: 2.297879\n",
      "Epoch 51, loss: 2.297603\n",
      "Epoch 52, loss: 2.298263\n",
      "Epoch 53, loss: 2.295780\n",
      "Epoch 54, loss: 2.296805\n",
      "Epoch 55, loss: 2.298900\n",
      "Epoch 56, loss: 2.294762\n",
      "Epoch 57, loss: 2.296570\n",
      "Epoch 58, loss: 2.298112\n",
      "Epoch 59, loss: 2.297004\n",
      "Epoch 60, loss: 2.295216\n",
      "Epoch 61, loss: 2.294887\n",
      "Epoch 62, loss: 2.295532\n",
      "Epoch 63, loss: 2.297541\n",
      "Epoch 64, loss: 2.293583\n",
      "Epoch 65, loss: 2.298998\n",
      "Epoch 66, loss: 2.295859\n",
      "Epoch 67, loss: 2.298288\n",
      "Epoch 68, loss: 2.292933\n",
      "Epoch 69, loss: 2.296862\n",
      "Epoch 70, loss: 2.294575\n",
      "Epoch 71, loss: 2.296397\n",
      "Epoch 72, loss: 2.294892\n",
      "Epoch 73, loss: 2.298685\n",
      "Epoch 74, loss: 2.294567\n",
      "Epoch 75, loss: 2.296488\n",
      "Epoch 76, loss: 2.295849\n",
      "Epoch 77, loss: 2.294594\n",
      "Epoch 78, loss: 2.294124\n",
      "Epoch 79, loss: 2.292552\n",
      "Epoch 80, loss: 2.293675\n",
      "Epoch 81, loss: 2.296621\n",
      "Epoch 82, loss: 2.295915\n",
      "Epoch 83, loss: 2.295305\n",
      "Epoch 84, loss: 2.293586\n",
      "Epoch 85, loss: 2.295619\n",
      "Epoch 86, loss: 2.291841\n",
      "Epoch 87, loss: 2.292239\n",
      "Epoch 88, loss: 2.290684\n",
      "Epoch 89, loss: 2.293103\n",
      "Epoch 90, loss: 2.296765\n",
      "Epoch 91, loss: 2.294367\n",
      "Epoch 92, loss: 2.296287\n",
      "Epoch 93, loss: 2.294887\n",
      "Epoch 94, loss: 2.291502\n",
      "Epoch 95, loss: 2.294612\n",
      "Epoch 96, loss: 2.294703\n",
      "Epoch 97, loss: 2.298774\n",
      "Epoch 98, loss: 2.296022\n",
      "Epoch 99, loss: 2.291963\n",
      "Epoch 100, loss: 2.293770\n",
      "Epoch 101, loss: 2.294066\n",
      "Epoch 102, loss: 2.291205\n",
      "Epoch 103, loss: 2.292831\n",
      "Epoch 104, loss: 2.295498\n",
      "Epoch 105, loss: 2.290582\n",
      "Epoch 106, loss: 2.295828\n",
      "Epoch 107, loss: 2.296568\n",
      "Epoch 108, loss: 2.292349\n",
      "Epoch 109, loss: 2.290034\n",
      "Epoch 110, loss: 2.288103\n",
      "Epoch 111, loss: 2.291917\n",
      "Epoch 112, loss: 2.295293\n",
      "Epoch 113, loss: 2.293358\n",
      "Epoch 114, loss: 2.293525\n",
      "Epoch 115, loss: 2.291149\n",
      "Epoch 116, loss: 2.291769\n",
      "Epoch 117, loss: 2.293633\n",
      "Epoch 118, loss: 2.294477\n",
      "Epoch 119, loss: 2.291958\n",
      "Epoch 120, loss: 2.293342\n",
      "Epoch 121, loss: 2.292307\n",
      "Epoch 122, loss: 2.288107\n",
      "Epoch 123, loss: 2.291391\n",
      "Epoch 124, loss: 2.290675\n",
      "Epoch 125, loss: 2.297625\n",
      "Epoch 126, loss: 2.284975\n",
      "Epoch 127, loss: 2.292975\n",
      "Epoch 128, loss: 2.295628\n",
      "Epoch 129, loss: 2.288985\n",
      "Epoch 130, loss: 2.286358\n",
      "Epoch 131, loss: 2.290820\n",
      "Epoch 132, loss: 2.290022\n",
      "Epoch 133, loss: 2.289443\n",
      "Epoch 134, loss: 2.294438\n",
      "Epoch 135, loss: 2.291419\n",
      "Epoch 136, loss: 2.293466\n",
      "Epoch 137, loss: 2.289945\n",
      "Epoch 138, loss: 2.291152\n",
      "Epoch 139, loss: 2.292801\n",
      "Epoch 140, loss: 2.290344\n",
      "Epoch 141, loss: 2.290861\n",
      "Epoch 142, loss: 2.290068\n",
      "Epoch 143, loss: 2.293475\n",
      "Epoch 144, loss: 2.289236\n",
      "Epoch 145, loss: 2.291304\n",
      "Epoch 146, loss: 2.290402\n",
      "Epoch 147, loss: 2.286446\n",
      "Epoch 148, loss: 2.288848\n",
      "Epoch 149, loss: 2.288777\n",
      "Epoch 150, loss: 2.291477\n",
      "Epoch 151, loss: 2.285855\n",
      "Epoch 152, loss: 2.287389\n",
      "Epoch 153, loss: 2.291269\n",
      "Epoch 154, loss: 2.284541\n",
      "Epoch 155, loss: 2.287171\n",
      "Epoch 156, loss: 2.289078\n",
      "Epoch 157, loss: 2.287364\n",
      "Epoch 158, loss: 2.288763\n",
      "Epoch 159, loss: 2.287981\n",
      "Epoch 160, loss: 2.288023\n",
      "Epoch 161, loss: 2.292352\n",
      "Epoch 162, loss: 2.288241\n",
      "Epoch 163, loss: 2.286306\n",
      "Epoch 164, loss: 2.291034\n",
      "Epoch 165, loss: 2.284612\n",
      "Epoch 166, loss: 2.291189\n",
      "Epoch 167, loss: 2.284186\n",
      "Epoch 168, loss: 2.285489\n",
      "Epoch 169, loss: 2.285284\n",
      "Epoch 170, loss: 2.285561\n",
      "Epoch 171, loss: 2.286339\n",
      "Epoch 172, loss: 2.289177\n",
      "Epoch 173, loss: 2.288402\n",
      "Epoch 174, loss: 2.287816\n",
      "Epoch 175, loss: 2.287579\n",
      "Epoch 176, loss: 2.284685\n",
      "Epoch 177, loss: 2.282518\n",
      "Epoch 178, loss: 2.290574\n",
      "Epoch 179, loss: 2.292186\n",
      "Epoch 180, loss: 2.283190\n",
      "Epoch 181, loss: 2.288784\n",
      "Epoch 182, loss: 2.285091\n",
      "Epoch 183, loss: 2.286017\n",
      "Epoch 184, loss: 2.287550\n",
      "Epoch 185, loss: 2.289593\n",
      "Epoch 186, loss: 2.282529\n",
      "Epoch 187, loss: 2.284871\n",
      "Epoch 188, loss: 2.287857\n",
      "Epoch 189, loss: 2.283141\n",
      "Epoch 190, loss: 2.287441\n",
      "Epoch 191, loss: 2.282360\n",
      "Epoch 192, loss: 2.289572\n",
      "Epoch 193, loss: 2.284836\n",
      "Epoch 194, loss: 2.285441\n",
      "Epoch 195, loss: 2.285944\n",
      "Epoch 196, loss: 2.283469\n",
      "Epoch 197, loss: 2.287642\n",
      "Epoch 198, loss: 2.286929\n",
      "Epoch 199, loss: 2.286377\n",
      "Epoch 0, loss: 2.301888\n",
      "Epoch 1, loss: 2.302803\n",
      "Epoch 2, loss: 2.302804\n",
      "Epoch 3, loss: 2.302705\n",
      "Epoch 4, loss: 2.302649\n",
      "Epoch 5, loss: 2.303385\n",
      "Epoch 6, loss: 2.301822\n",
      "Epoch 7, loss: 2.303277\n",
      "Epoch 8, loss: 2.302536\n",
      "Epoch 9, loss: 2.302537\n",
      "Epoch 10, loss: 2.301641\n",
      "Epoch 11, loss: 2.302659\n",
      "Epoch 12, loss: 2.302785\n",
      "Epoch 13, loss: 2.302638\n",
      "Epoch 14, loss: 2.302911\n",
      "Epoch 15, loss: 2.301443\n",
      "Epoch 16, loss: 2.303244\n",
      "Epoch 17, loss: 2.303408\n",
      "Epoch 18, loss: 2.303059\n",
      "Epoch 19, loss: 2.302554\n",
      "Epoch 20, loss: 2.302286\n",
      "Epoch 21, loss: 2.303241\n",
      "Epoch 22, loss: 2.301972\n",
      "Epoch 23, loss: 2.302546\n",
      "Epoch 24, loss: 2.302033\n",
      "Epoch 25, loss: 2.302405\n",
      "Epoch 26, loss: 2.302516\n",
      "Epoch 27, loss: 2.302586\n",
      "Epoch 28, loss: 2.302126\n",
      "Epoch 29, loss: 2.302077\n",
      "Epoch 30, loss: 2.301932\n",
      "Epoch 31, loss: 2.302677\n",
      "Epoch 32, loss: 2.303515\n",
      "Epoch 33, loss: 2.302440\n",
      "Epoch 34, loss: 2.303137\n",
      "Epoch 35, loss: 2.302017\n",
      "Epoch 36, loss: 2.301864\n",
      "Epoch 37, loss: 2.302775\n",
      "Epoch 38, loss: 2.302884\n",
      "Epoch 39, loss: 2.301028\n",
      "Epoch 40, loss: 2.302069\n",
      "Epoch 41, loss: 2.301726\n",
      "Epoch 42, loss: 2.301906\n",
      "Epoch 43, loss: 2.302719\n",
      "Epoch 44, loss: 2.302164\n",
      "Epoch 45, loss: 2.301461\n",
      "Epoch 46, loss: 2.301460\n",
      "Epoch 47, loss: 2.302893\n",
      "Epoch 48, loss: 2.301526\n",
      "Epoch 49, loss: 2.302445\n",
      "Epoch 50, loss: 2.302178\n",
      "Epoch 51, loss: 2.301767\n",
      "Epoch 52, loss: 2.301269\n",
      "Epoch 53, loss: 2.303138\n",
      "Epoch 54, loss: 2.301491\n",
      "Epoch 55, loss: 2.300768\n",
      "Epoch 56, loss: 2.301981\n",
      "Epoch 57, loss: 2.301420\n",
      "Epoch 58, loss: 2.302664\n",
      "Epoch 59, loss: 2.302935\n",
      "Epoch 60, loss: 2.301230\n",
      "Epoch 61, loss: 2.301522\n",
      "Epoch 62, loss: 2.302775\n",
      "Epoch 63, loss: 2.302581\n",
      "Epoch 64, loss: 2.301936\n",
      "Epoch 65, loss: 2.301091\n",
      "Epoch 66, loss: 2.302935\n",
      "Epoch 67, loss: 2.303007\n",
      "Epoch 68, loss: 2.301400\n",
      "Epoch 69, loss: 2.302041\n",
      "Epoch 70, loss: 2.302342\n",
      "Epoch 71, loss: 2.301488\n",
      "Epoch 72, loss: 2.301926\n",
      "Epoch 73, loss: 2.302400\n",
      "Epoch 74, loss: 2.301022\n",
      "Epoch 75, loss: 2.301413\n",
      "Epoch 76, loss: 2.301474\n",
      "Epoch 77, loss: 2.301946\n",
      "Epoch 78, loss: 2.301585\n",
      "Epoch 79, loss: 2.300485\n",
      "Epoch 80, loss: 2.302481\n",
      "Epoch 81, loss: 2.301665\n",
      "Epoch 82, loss: 2.302147\n",
      "Epoch 83, loss: 2.301635\n",
      "Epoch 84, loss: 2.300948\n",
      "Epoch 85, loss: 2.301225\n",
      "Epoch 86, loss: 2.301106\n",
      "Epoch 87, loss: 2.302177\n",
      "Epoch 88, loss: 2.301870\n",
      "Epoch 89, loss: 2.302301\n",
      "Epoch 90, loss: 2.301189\n",
      "Epoch 91, loss: 2.301309\n",
      "Epoch 92, loss: 2.301718\n",
      "Epoch 93, loss: 2.302704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94, loss: 2.302304\n",
      "Epoch 95, loss: 2.301090\n",
      "Epoch 96, loss: 2.301631\n",
      "Epoch 97, loss: 2.302004\n",
      "Epoch 98, loss: 2.301276\n",
      "Epoch 99, loss: 2.301358\n",
      "Epoch 100, loss: 2.301265\n",
      "Epoch 101, loss: 2.302163\n",
      "Epoch 102, loss: 2.301310\n",
      "Epoch 103, loss: 2.301126\n",
      "Epoch 104, loss: 2.301277\n",
      "Epoch 105, loss: 2.302362\n",
      "Epoch 106, loss: 2.301593\n",
      "Epoch 107, loss: 2.301462\n",
      "Epoch 108, loss: 2.302452\n",
      "Epoch 109, loss: 2.302349\n",
      "Epoch 110, loss: 2.300532\n",
      "Epoch 111, loss: 2.300761\n",
      "Epoch 112, loss: 2.301019\n",
      "Epoch 113, loss: 2.300554\n",
      "Epoch 114, loss: 2.300401\n",
      "Epoch 115, loss: 2.301676\n",
      "Epoch 116, loss: 2.302088\n",
      "Epoch 117, loss: 2.301514\n",
      "Epoch 118, loss: 2.301605\n",
      "Epoch 119, loss: 2.301993\n",
      "Epoch 120, loss: 2.302446\n",
      "Epoch 121, loss: 2.303151\n",
      "Epoch 122, loss: 2.301022\n",
      "Epoch 123, loss: 2.301568\n",
      "Epoch 124, loss: 2.301296\n",
      "Epoch 125, loss: 2.302223\n",
      "Epoch 126, loss: 2.302829\n",
      "Epoch 127, loss: 2.302186\n",
      "Epoch 128, loss: 2.300724\n",
      "Epoch 129, loss: 2.300979\n",
      "Epoch 130, loss: 2.302279\n",
      "Epoch 131, loss: 2.300683\n",
      "Epoch 132, loss: 2.301268\n",
      "Epoch 133, loss: 2.301494\n",
      "Epoch 134, loss: 2.300601\n",
      "Epoch 135, loss: 2.301180\n",
      "Epoch 136, loss: 2.301353\n",
      "Epoch 137, loss: 2.300731\n",
      "Epoch 138, loss: 2.300765\n",
      "Epoch 139, loss: 2.301364\n",
      "Epoch 140, loss: 2.301738\n",
      "Epoch 141, loss: 2.299722\n",
      "Epoch 142, loss: 2.302753\n",
      "Epoch 143, loss: 2.301987\n",
      "Epoch 144, loss: 2.301019\n",
      "Epoch 145, loss: 2.301155\n",
      "Epoch 146, loss: 2.300136\n",
      "Epoch 147, loss: 2.300810\n",
      "Epoch 148, loss: 2.302389\n",
      "Epoch 149, loss: 2.301845\n",
      "Epoch 150, loss: 2.300617\n",
      "Epoch 151, loss: 2.300346\n",
      "Epoch 152, loss: 2.300793\n",
      "Epoch 153, loss: 2.300622\n",
      "Epoch 154, loss: 2.301759\n",
      "Epoch 155, loss: 2.300422\n",
      "Epoch 156, loss: 2.301191\n",
      "Epoch 157, loss: 2.300986\n",
      "Epoch 158, loss: 2.301513\n",
      "Epoch 159, loss: 2.300270\n",
      "Epoch 160, loss: 2.300344\n",
      "Epoch 161, loss: 2.300072\n",
      "Epoch 162, loss: 2.300239\n",
      "Epoch 163, loss: 2.299933\n",
      "Epoch 164, loss: 2.301690\n",
      "Epoch 165, loss: 2.300311\n",
      "Epoch 166, loss: 2.301122\n",
      "Epoch 167, loss: 2.300821\n",
      "Epoch 168, loss: 2.301353\n",
      "Epoch 169, loss: 2.301315\n",
      "Epoch 170, loss: 2.300395\n",
      "Epoch 171, loss: 2.300856\n",
      "Epoch 172, loss: 2.299837\n",
      "Epoch 173, loss: 2.301074\n",
      "Epoch 174, loss: 2.300972\n",
      "Epoch 175, loss: 2.299189\n",
      "Epoch 176, loss: 2.301072\n",
      "Epoch 177, loss: 2.300158\n",
      "Epoch 178, loss: 2.299067\n",
      "Epoch 179, loss: 2.300887\n",
      "Epoch 180, loss: 2.301236\n",
      "Epoch 181, loss: 2.301755\n",
      "Epoch 182, loss: 2.300947\n",
      "Epoch 183, loss: 2.301529\n",
      "Epoch 184, loss: 2.301313\n",
      "Epoch 185, loss: 2.299597\n",
      "Epoch 186, loss: 2.300958\n",
      "Epoch 187, loss: 2.301047\n",
      "Epoch 188, loss: 2.300277\n",
      "Epoch 189, loss: 2.299644\n",
      "Epoch 190, loss: 2.300806\n",
      "Epoch 191, loss: 2.300462\n",
      "Epoch 192, loss: 2.298689\n",
      "Epoch 193, loss: 2.302019\n",
      "Epoch 194, loss: 2.301071\n",
      "Epoch 195, loss: 2.300725\n",
      "Epoch 196, loss: 2.301066\n",
      "Epoch 197, loss: 2.301008\n",
      "Epoch 198, loss: 2.300919\n",
      "Epoch 199, loss: 2.300451\n",
      "Epoch 0, loss: 2.302773\n",
      "Epoch 1, loss: 2.303589\n",
      "Epoch 2, loss: 2.302858\n",
      "Epoch 3, loss: 2.301939\n",
      "Epoch 4, loss: 2.301979\n",
      "Epoch 5, loss: 2.302073\n",
      "Epoch 6, loss: 2.302608\n",
      "Epoch 7, loss: 2.302581\n",
      "Epoch 8, loss: 2.302897\n",
      "Epoch 9, loss: 2.302204\n",
      "Epoch 10, loss: 2.302497\n",
      "Epoch 11, loss: 2.302610\n",
      "Epoch 12, loss: 2.302573\n",
      "Epoch 13, loss: 2.302658\n",
      "Epoch 14, loss: 2.302786\n",
      "Epoch 15, loss: 2.302962\n",
      "Epoch 16, loss: 2.302433\n",
      "Epoch 17, loss: 2.302875\n",
      "Epoch 18, loss: 2.303079\n",
      "Epoch 19, loss: 2.302247\n",
      "Epoch 20, loss: 2.302802\n",
      "Epoch 21, loss: 2.302950\n",
      "Epoch 22, loss: 2.302000\n",
      "Epoch 23, loss: 2.302969\n",
      "Epoch 24, loss: 2.301207\n",
      "Epoch 25, loss: 2.302632\n",
      "Epoch 26, loss: 2.303840\n",
      "Epoch 27, loss: 2.302147\n",
      "Epoch 28, loss: 2.302499\n",
      "Epoch 29, loss: 2.302526\n",
      "Epoch 30, loss: 2.302398\n",
      "Epoch 31, loss: 2.301774\n",
      "Epoch 32, loss: 2.302388\n",
      "Epoch 33, loss: 2.302934\n",
      "Epoch 34, loss: 2.301595\n",
      "Epoch 35, loss: 2.301918\n",
      "Epoch 36, loss: 2.302423\n",
      "Epoch 37, loss: 2.302182\n",
      "Epoch 38, loss: 2.303679\n",
      "Epoch 39, loss: 2.302293\n",
      "Epoch 40, loss: 2.301506\n",
      "Epoch 41, loss: 2.302201\n",
      "Epoch 42, loss: 2.302302\n",
      "Epoch 43, loss: 2.302936\n",
      "Epoch 44, loss: 2.301978\n",
      "Epoch 45, loss: 2.302600\n",
      "Epoch 46, loss: 2.302493\n",
      "Epoch 47, loss: 2.301949\n",
      "Epoch 48, loss: 2.301099\n",
      "Epoch 49, loss: 2.301902\n",
      "Epoch 50, loss: 2.301295\n",
      "Epoch 51, loss: 2.302327\n",
      "Epoch 52, loss: 2.302361\n",
      "Epoch 53, loss: 2.301944\n",
      "Epoch 54, loss: 2.301648\n",
      "Epoch 55, loss: 2.300560\n",
      "Epoch 56, loss: 2.301815\n",
      "Epoch 57, loss: 2.301108\n",
      "Epoch 58, loss: 2.301674\n",
      "Epoch 59, loss: 2.301081\n",
      "Epoch 60, loss: 2.301973\n",
      "Epoch 61, loss: 2.301390\n",
      "Epoch 62, loss: 2.301985\n",
      "Epoch 63, loss: 2.301817\n",
      "Epoch 64, loss: 2.301854\n",
      "Epoch 65, loss: 2.301503\n",
      "Epoch 66, loss: 2.302716\n",
      "Epoch 67, loss: 2.302286\n",
      "Epoch 68, loss: 2.302478\n",
      "Epoch 69, loss: 2.301609\n",
      "Epoch 70, loss: 2.302362\n",
      "Epoch 71, loss: 2.300996\n",
      "Epoch 72, loss: 2.301761\n",
      "Epoch 73, loss: 2.301077\n",
      "Epoch 74, loss: 2.302502\n",
      "Epoch 75, loss: 2.301074\n",
      "Epoch 76, loss: 2.303046\n",
      "Epoch 77, loss: 2.301484\n",
      "Epoch 78, loss: 2.301561\n",
      "Epoch 79, loss: 2.302319\n",
      "Epoch 80, loss: 2.301274\n",
      "Epoch 81, loss: 2.303301\n",
      "Epoch 82, loss: 2.302512\n",
      "Epoch 83, loss: 2.301080\n",
      "Epoch 84, loss: 2.301477\n",
      "Epoch 85, loss: 2.302462\n",
      "Epoch 86, loss: 2.302495\n",
      "Epoch 87, loss: 2.301829\n",
      "Epoch 88, loss: 2.301764\n",
      "Epoch 89, loss: 2.301190\n",
      "Epoch 90, loss: 2.301562\n",
      "Epoch 91, loss: 2.301842\n",
      "Epoch 92, loss: 2.299485\n",
      "Epoch 93, loss: 2.302126\n",
      "Epoch 94, loss: 2.301478\n",
      "Epoch 95, loss: 2.301588\n",
      "Epoch 96, loss: 2.302086\n",
      "Epoch 97, loss: 2.301243\n",
      "Epoch 98, loss: 2.301161\n",
      "Epoch 99, loss: 2.301146\n",
      "Epoch 100, loss: 2.300876\n",
      "Epoch 101, loss: 2.302186\n",
      "Epoch 102, loss: 2.302036\n",
      "Epoch 103, loss: 2.301431\n",
      "Epoch 104, loss: 2.300859\n",
      "Epoch 105, loss: 2.301374\n",
      "Epoch 106, loss: 2.300591\n",
      "Epoch 107, loss: 2.301434\n",
      "Epoch 108, loss: 2.301639\n",
      "Epoch 109, loss: 2.302314\n",
      "Epoch 110, loss: 2.301965\n",
      "Epoch 111, loss: 2.300751\n",
      "Epoch 112, loss: 2.301107\n",
      "Epoch 113, loss: 2.300309\n",
      "Epoch 114, loss: 2.301958\n",
      "Epoch 115, loss: 2.300573\n",
      "Epoch 116, loss: 2.301583\n",
      "Epoch 117, loss: 2.301448\n",
      "Epoch 118, loss: 2.301840\n",
      "Epoch 119, loss: 2.300277\n",
      "Epoch 120, loss: 2.301396\n",
      "Epoch 121, loss: 2.301159\n",
      "Epoch 122, loss: 2.300103\n",
      "Epoch 123, loss: 2.300664\n",
      "Epoch 124, loss: 2.301787\n",
      "Epoch 125, loss: 2.300443\n",
      "Epoch 126, loss: 2.302484\n",
      "Epoch 127, loss: 2.302102\n",
      "Epoch 128, loss: 2.301871\n",
      "Epoch 129, loss: 2.301783\n",
      "Epoch 130, loss: 2.300427\n",
      "Epoch 131, loss: 2.300956\n",
      "Epoch 132, loss: 2.301135\n",
      "Epoch 133, loss: 2.300781\n",
      "Epoch 134, loss: 2.301626\n",
      "Epoch 135, loss: 2.300571\n",
      "Epoch 136, loss: 2.301812\n",
      "Epoch 137, loss: 2.302302\n",
      "Epoch 138, loss: 2.301425\n",
      "Epoch 139, loss: 2.301056\n",
      "Epoch 140, loss: 2.302710\n",
      "Epoch 141, loss: 2.300649\n",
      "Epoch 142, loss: 2.302937\n",
      "Epoch 143, loss: 2.300785\n",
      "Epoch 144, loss: 2.300361\n",
      "Epoch 145, loss: 2.301800\n",
      "Epoch 146, loss: 2.301052\n",
      "Epoch 147, loss: 2.300450\n",
      "Epoch 148, loss: 2.300544\n",
      "Epoch 149, loss: 2.300557\n",
      "Epoch 150, loss: 2.300807\n",
      "Epoch 151, loss: 2.301574\n",
      "Epoch 152, loss: 2.300712\n",
      "Epoch 153, loss: 2.301307\n",
      "Epoch 154, loss: 2.300349\n",
      "Epoch 155, loss: 2.299678\n",
      "Epoch 156, loss: 2.301463\n",
      "Epoch 157, loss: 2.300608\n",
      "Epoch 158, loss: 2.300963\n",
      "Epoch 159, loss: 2.299700\n",
      "Epoch 160, loss: 2.299943\n",
      "Epoch 161, loss: 2.300622\n",
      "Epoch 162, loss: 2.300970\n",
      "Epoch 163, loss: 2.300802\n",
      "Epoch 164, loss: 2.299677\n",
      "Epoch 165, loss: 2.300947\n",
      "Epoch 166, loss: 2.301885\n",
      "Epoch 167, loss: 2.300842\n",
      "Epoch 168, loss: 2.300447\n",
      "Epoch 169, loss: 2.301285\n",
      "Epoch 170, loss: 2.301017\n",
      "Epoch 171, loss: 2.299513\n",
      "Epoch 172, loss: 2.300843\n",
      "Epoch 173, loss: 2.301203\n",
      "Epoch 174, loss: 2.301571\n",
      "Epoch 175, loss: 2.301499\n",
      "Epoch 176, loss: 2.300016\n",
      "Epoch 177, loss: 2.300053\n",
      "Epoch 178, loss: 2.300988\n",
      "Epoch 179, loss: 2.301232\n",
      "Epoch 180, loss: 2.300993\n",
      "Epoch 181, loss: 2.301097\n",
      "Epoch 182, loss: 2.300252\n",
      "Epoch 183, loss: 2.299908\n",
      "Epoch 184, loss: 2.301781\n",
      "Epoch 185, loss: 2.300742\n",
      "Epoch 186, loss: 2.300216\n",
      "Epoch 187, loss: 2.299424\n",
      "Epoch 188, loss: 2.300391\n",
      "Epoch 189, loss: 2.301343\n",
      "Epoch 190, loss: 2.299797\n",
      "Epoch 191, loss: 2.299879\n",
      "Epoch 192, loss: 2.300926\n",
      "Epoch 193, loss: 2.300967\n",
      "Epoch 194, loss: 2.300133\n",
      "Epoch 195, loss: 2.300316\n",
      "Epoch 196, loss: 2.301341\n",
      "Epoch 197, loss: 2.302406\n",
      "Epoch 198, loss: 2.300113\n",
      "Epoch 199, loss: 2.300887\n",
      "Epoch 0, loss: 2.302133\n",
      "Epoch 1, loss: 2.302946\n",
      "Epoch 2, loss: 2.302879\n",
      "Epoch 3, loss: 2.303658\n",
      "Epoch 4, loss: 2.303317\n",
      "Epoch 5, loss: 2.301630\n",
      "Epoch 6, loss: 2.301660\n",
      "Epoch 7, loss: 2.302037\n",
      "Epoch 8, loss: 2.302724\n",
      "Epoch 9, loss: 2.301360\n",
      "Epoch 10, loss: 2.302859\n",
      "Epoch 11, loss: 2.302945\n",
      "Epoch 12, loss: 2.302742\n",
      "Epoch 13, loss: 2.301668\n",
      "Epoch 14, loss: 2.301769\n",
      "Epoch 15, loss: 2.302659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, loss: 2.302973\n",
      "Epoch 17, loss: 2.301399\n",
      "Epoch 18, loss: 2.302478\n",
      "Epoch 19, loss: 2.302659\n",
      "Epoch 20, loss: 2.302796\n",
      "Epoch 21, loss: 2.302164\n",
      "Epoch 22, loss: 2.302594\n",
      "Epoch 23, loss: 2.303413\n",
      "Epoch 24, loss: 2.302766\n",
      "Epoch 25, loss: 2.302437\n",
      "Epoch 26, loss: 2.302886\n",
      "Epoch 27, loss: 2.302055\n",
      "Epoch 28, loss: 2.302725\n",
      "Epoch 29, loss: 2.302417\n",
      "Epoch 30, loss: 2.301905\n",
      "Epoch 31, loss: 2.302062\n",
      "Epoch 32, loss: 2.302047\n",
      "Epoch 33, loss: 2.301432\n",
      "Epoch 34, loss: 2.300573\n",
      "Epoch 35, loss: 2.301218\n",
      "Epoch 36, loss: 2.302336\n",
      "Epoch 37, loss: 2.302672\n",
      "Epoch 38, loss: 2.301824\n",
      "Epoch 39, loss: 2.301961\n",
      "Epoch 40, loss: 2.301638\n",
      "Epoch 41, loss: 2.302695\n",
      "Epoch 42, loss: 2.302018\n",
      "Epoch 43, loss: 2.302044\n",
      "Epoch 44, loss: 2.301666\n",
      "Epoch 45, loss: 2.301678\n",
      "Epoch 46, loss: 2.301064\n",
      "Epoch 47, loss: 2.302460\n",
      "Epoch 48, loss: 2.301307\n",
      "Epoch 49, loss: 2.302604\n",
      "Epoch 50, loss: 2.302322\n",
      "Epoch 51, loss: 2.301969\n",
      "Epoch 52, loss: 2.302229\n",
      "Epoch 53, loss: 2.301945\n",
      "Epoch 54, loss: 2.301427\n",
      "Epoch 55, loss: 2.301778\n",
      "Epoch 56, loss: 2.302620\n",
      "Epoch 57, loss: 2.302596\n",
      "Epoch 58, loss: 2.301650\n",
      "Epoch 59, loss: 2.300870\n",
      "Epoch 60, loss: 2.302028\n",
      "Epoch 61, loss: 2.301341\n",
      "Epoch 62, loss: 2.302473\n",
      "Epoch 63, loss: 2.302525\n",
      "Epoch 64, loss: 2.301349\n",
      "Epoch 65, loss: 2.302038\n",
      "Epoch 66, loss: 2.301373\n",
      "Epoch 67, loss: 2.302054\n",
      "Epoch 68, loss: 2.302716\n",
      "Epoch 69, loss: 2.302699\n",
      "Epoch 70, loss: 2.301188\n",
      "Epoch 71, loss: 2.301854\n",
      "Epoch 72, loss: 2.300227\n",
      "Epoch 73, loss: 2.302826\n",
      "Epoch 74, loss: 2.302543\n",
      "Epoch 75, loss: 2.302170\n",
      "Epoch 76, loss: 2.301719\n",
      "Epoch 77, loss: 2.302178\n",
      "Epoch 78, loss: 2.301143\n",
      "Epoch 79, loss: 2.301661\n",
      "Epoch 80, loss: 2.301392\n",
      "Epoch 81, loss: 2.302269\n",
      "Epoch 82, loss: 2.301208\n",
      "Epoch 83, loss: 2.302121\n",
      "Epoch 84, loss: 2.301947\n",
      "Epoch 85, loss: 2.301060\n",
      "Epoch 86, loss: 2.301999\n",
      "Epoch 87, loss: 2.301576\n",
      "Epoch 88, loss: 2.302010\n",
      "Epoch 89, loss: 2.302051\n",
      "Epoch 90, loss: 2.303637\n",
      "Epoch 91, loss: 2.301180\n",
      "Epoch 92, loss: 2.302559\n",
      "Epoch 93, loss: 2.301275\n",
      "Epoch 94, loss: 2.302481\n",
      "Epoch 95, loss: 2.300723\n",
      "Epoch 96, loss: 2.302065\n",
      "Epoch 97, loss: 2.301039\n",
      "Epoch 98, loss: 2.302193\n",
      "Epoch 99, loss: 2.300702\n",
      "Epoch 100, loss: 2.300072\n",
      "Epoch 101, loss: 2.301111\n",
      "Epoch 102, loss: 2.300737\n",
      "Epoch 103, loss: 2.302365\n",
      "Epoch 104, loss: 2.302458\n",
      "Epoch 105, loss: 2.301854\n",
      "Epoch 106, loss: 2.300969\n",
      "Epoch 107, loss: 2.301381\n",
      "Epoch 108, loss: 2.301477\n",
      "Epoch 109, loss: 2.301073\n",
      "Epoch 110, loss: 2.300156\n",
      "Epoch 111, loss: 2.302292\n",
      "Epoch 112, loss: 2.301564\n",
      "Epoch 113, loss: 2.300803\n",
      "Epoch 114, loss: 2.301155\n",
      "Epoch 115, loss: 2.300939\n",
      "Epoch 116, loss: 2.302361\n",
      "Epoch 117, loss: 2.301491\n",
      "Epoch 118, loss: 2.301171\n",
      "Epoch 119, loss: 2.301956\n",
      "Epoch 120, loss: 2.301783\n",
      "Epoch 121, loss: 2.302048\n",
      "Epoch 122, loss: 2.301398\n",
      "Epoch 123, loss: 2.301289\n",
      "Epoch 124, loss: 2.300231\n",
      "Epoch 125, loss: 2.301546\n",
      "Epoch 126, loss: 2.300554\n",
      "Epoch 127, loss: 2.300114\n",
      "Epoch 128, loss: 2.302071\n",
      "Epoch 129, loss: 2.300844\n",
      "Epoch 130, loss: 2.300342\n",
      "Epoch 131, loss: 2.300692\n",
      "Epoch 132, loss: 2.300953\n",
      "Epoch 133, loss: 2.301220\n",
      "Epoch 134, loss: 2.301030\n",
      "Epoch 135, loss: 2.301118\n",
      "Epoch 136, loss: 2.300377\n",
      "Epoch 137, loss: 2.302856\n",
      "Epoch 138, loss: 2.301435\n",
      "Epoch 139, loss: 2.300849\n",
      "Epoch 140, loss: 2.302030\n",
      "Epoch 141, loss: 2.301404\n",
      "Epoch 142, loss: 2.301772\n",
      "Epoch 143, loss: 2.300886\n",
      "Epoch 144, loss: 2.299714\n",
      "Epoch 145, loss: 2.300652\n",
      "Epoch 146, loss: 2.301717\n",
      "Epoch 147, loss: 2.302366\n",
      "Epoch 148, loss: 2.300469\n",
      "Epoch 149, loss: 2.301069\n",
      "Epoch 150, loss: 2.300997\n",
      "Epoch 151, loss: 2.299688\n",
      "Epoch 152, loss: 2.301764\n",
      "Epoch 153, loss: 2.300720\n",
      "Epoch 154, loss: 2.301394\n",
      "Epoch 155, loss: 2.300619\n",
      "Epoch 156, loss: 2.301488\n",
      "Epoch 157, loss: 2.300965\n",
      "Epoch 158, loss: 2.301849\n",
      "Epoch 159, loss: 2.300929\n",
      "Epoch 160, loss: 2.300965\n",
      "Epoch 161, loss: 2.302019\n",
      "Epoch 162, loss: 2.300756\n",
      "Epoch 163, loss: 2.299450\n",
      "Epoch 164, loss: 2.300041\n",
      "Epoch 165, loss: 2.300019\n",
      "Epoch 166, loss: 2.301070\n",
      "Epoch 167, loss: 2.300386\n",
      "Epoch 168, loss: 2.299439\n",
      "Epoch 169, loss: 2.300282\n",
      "Epoch 170, loss: 2.300897\n",
      "Epoch 171, loss: 2.301880\n",
      "Epoch 172, loss: 2.300200\n",
      "Epoch 173, loss: 2.300663\n",
      "Epoch 174, loss: 2.300318\n",
      "Epoch 175, loss: 2.302247\n",
      "Epoch 176, loss: 2.300262\n",
      "Epoch 177, loss: 2.300075\n",
      "Epoch 178, loss: 2.299128\n",
      "Epoch 179, loss: 2.301732\n",
      "Epoch 180, loss: 2.298706\n",
      "Epoch 181, loss: 2.300820\n",
      "Epoch 182, loss: 2.301458\n",
      "Epoch 183, loss: 2.300143\n",
      "Epoch 184, loss: 2.301892\n",
      "Epoch 185, loss: 2.300427\n",
      "Epoch 186, loss: 2.300392\n",
      "Epoch 187, loss: 2.300208\n",
      "Epoch 188, loss: 2.301508\n",
      "Epoch 189, loss: 2.301971\n",
      "Epoch 190, loss: 2.300396\n",
      "Epoch 191, loss: 2.301389\n",
      "Epoch 192, loss: 2.300605\n",
      "Epoch 193, loss: 2.300721\n",
      "Epoch 194, loss: 2.299770\n",
      "Epoch 195, loss: 2.299451\n",
      "Epoch 196, loss: 2.300245\n",
      "Epoch 197, loss: 2.301016\n",
      "Epoch 198, loss: 2.301587\n",
      "Epoch 199, loss: 2.299325\n",
      "best validation accuracy achieved: 0.230000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "reg_strengths = [1e-4, 1e-5, 1e-6]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = 0\n",
    "\n",
    "# TODO use validation set to find the best hyperparameters\n",
    "# hint: for best results, you might need to try more values for learning rate and regularization strength \n",
    "# than provided initially\n",
    "for lr in learning_rates:\n",
    "    for rs in reg_strengths:\n",
    "        classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "        loss_history = classifier.fit(train_X, train_y, epochs=num_epochs,\n",
    "                                      learning_rate=lr, batch_size=batch_size, reg=rs)\n",
    "        pred = classifier.predict(val_X)\n",
    "        accuracy = multiclass_accuracy(pred, val_y)\n",
    "        if accuracy > best_val_accuracy:\n",
    "            best_classifier = classifier\n",
    "            best_val_accuracy = accuracy\n",
    "            best_lr, best_rs = lr, rs\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear softmax classifier test set accuracy: 0.195000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
