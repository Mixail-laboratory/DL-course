{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2.1 - Нейронные сети\n",
    "\n",
    "В этом задании вы реализуете и натренируете настоящую нейроную сеть своими руками!\n",
    "\n",
    "В некотором смысле это будет расширением прошлого задания - нам нужно просто составить несколько линейных классификаторов вместе!\n",
    "\n",
    "<img src=\"https://i.redd.it/n9fgba8b0qr01.png\" alt=\"Stack_more_layers\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_layer_gradient, check_layer_param_gradient, check_model_gradient\n",
    "from layers import FullyConnectedLayer, ReLULayer\n",
    "from model import TwoLayerNet\n",
    "from trainer import Trainer, Dataset\n",
    "from optim import SGD, MomentumSGD\n",
    "from metrics import multiclass_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загружаем данные\n",
    "\n",
    "И разделяем их на training и validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26934/2810386256.py:2: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
      "/tmp/ipykernel_26934/2810386256.py:3: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n"
     ]
    }
   ],
   "source": [
    "def prepare_for_neural_network(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    return train_flat, test_flat\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_neural_network(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, начинаем с кирпичиков\n",
    "\n",
    "Мы будем реализовывать необходимые нам слои по очереди. Каждый слой должен реализовать:\n",
    "- прямой проход (forward pass), который генерирует выход слоя по входу и запоминает необходимые данные\n",
    "- обратный проход (backward pass), который получает градиент по выходу слоя и вычисляет градиент по входу и по параметрам\n",
    "\n",
    "Начнем с ReLU, у которого параметров нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement ReLULayer layer in layers.py\n",
    "# Note: you'll need to copy implementation of the gradient_check function from the previous assignment\n",
    "\n",
    "X = np.array([[1,-2,3],\n",
    "              [-1, 2, 0.1]\n",
    "              ])\n",
    "\n",
    "assert check_layer_gradient(ReLULayer(), X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь реализуем полносвязный слой (fully connected layer), у которого будет два массива параметров: W (weights) и B (bias).\n",
    "\n",
    "Все параметры наши слои будут использовать для параметров специальный класс `Param`, в котором будут храниться значения параметров и градиенты этих параметров, вычисляемые во время обратного прохода.\n",
    "\n",
    "Это даст возможность аккумулировать (суммировать) градиенты из разных частей функции потерь, например, из cross-entropy loss и regularization loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement FullyConnected layer forward and backward methods\n",
    "assert check_layer_gradient(FullyConnectedLayer(3, 4), X)\n",
    "# TODO: Implement storing gradients for W and B\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'W')\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем нейронную сеть\n",
    "\n",
    "Теперь мы реализуем простейшую нейронную сеть с двумя полносвязным слоями и нелинейностью ReLU. Реализуйте функцию `compute_loss_and_gradients`, она должна запустить прямой и обратный проход через оба слоя для вычисления градиентов.\n",
    "\n",
    "Не забудьте реализовать очистку градиентов в начале функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for W_out\n",
      "Gradient check passed!\n",
      "Checking gradient for W_in\n",
      "Gradient check passed!\n",
      "Checking gradient for B_out\n",
      "Gradient check passed!\n",
      "Checking gradient for B_in\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: In model.py, implement compute_loss_and_gradients function\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 0)\n",
    "loss = model.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "\n",
    "# TODO Now implement backward pass and aggregate all of the params\n",
    "check_model_gradient(model, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавьте к модели регуляризацию - она должна прибавляться к loss и делать свой вклад в градиенты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for W_out\n",
      "Gradient check passed!\n",
      "Checking gradient for W_in\n",
      "Gradient check passed!\n",
      "Checking gradient for B_out\n",
      "Gradient check passed!\n",
      "Checking gradient for B_in\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Now implement l2 regularization in the forward and backward pass\n",
    "model_with_reg = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 1e1)\n",
    "loss_with_reg = model_with_reg.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "assert loss_with_reg > loss and not np.isclose(loss_with_reg, loss), \\\n",
    "    \"Loss with regularization (%2.4f) should be higher than without it (%2.4f)!\" % (loss, loss_with_reg)\n",
    "\n",
    "check_model_gradient(model_with_reg, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также реализуем функцию предсказания (вычисления значения) модели на новых данных.\n",
    "\n",
    "Какое значение точности мы ожидаем увидеть до начала тренировки?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16666666666666666"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, implement predict function!\n",
    "\n",
    "# TODO: Implement predict function\n",
    "# What would be the value we expect?\n",
    "multiclass_accuracy(model_with_reg.predict(train_X[:30]), train_y[:30]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Допишем код для процесса тренировки\n",
    "\n",
    "Если все реализовано корректно, значение функции ошибки должно уменьшаться с каждой эпохой, пусть и медленно. Не беспокойтесь пока про validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate = 1e-2)\n",
    "\n",
    "# TODO Implement missing pieces in Trainer.fit function\n",
    "# You should expect loss to go down every epoch, even if it's slow\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Улучшаем процесс тренировки\n",
    "\n",
    "Мы реализуем несколько ключевых оптимизаций, необходимых для тренировки современных нейросетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Уменьшение скорости обучения (learning rate decay)\n",
    "\n",
    "Одна из необходимых оптимизаций во время тренировки нейронных сетей - постепенное уменьшение скорости обучения по мере тренировки.\n",
    "\n",
    "Один из стандартных методов - уменьшение скорости обучения (learning rate) каждые N эпох на коэффициент d (часто называемый decay). Значения N и d, как всегда, являются гиперпараметрами и должны подбираться на основе эффективности на проверочных данных (validation data). \n",
    "\n",
    "В нашем случае N будет равным 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.249928, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.239042, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.298346, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.213825, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.271583, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.253327, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.185414, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.252596, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.303797, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.378797, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.331100, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.233551, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.263052, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.314094, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.225365, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.340751, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.285382, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.345458, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.130736, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.233158, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    }
   ],
   "source": [
    "# TODO Implement learning rate decay inside Trainer.fit method\n",
    "# Decay should happen once per epoch\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate_decay=0.99)\n",
    "\n",
    "initial_learning_rate = trainer.learning_rate\n",
    "loss_history, train_history, val_history = trainer.fit()\n",
    "\n",
    "assert trainer.learning_rate < initial_learning_rate, \"Learning rate should've been reduced\"\n",
    "assert trainer.learning_rate > 0.5*initial_learning_rate, \"Learning rate shouldn'tve been reduced that much!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Накопление импульса (Momentum SGD)\n",
    "\n",
    "Другой большой класс оптимизаций - использование более эффективных методов градиентного спуска. Мы реализуем один из них - накопление импульса (Momentum SGD).\n",
    "\n",
    "Этот метод хранит скорость движения, использует градиент для ее изменения на каждом шаге, и изменяет веса пропорционально значению скорости.\n",
    "(Физическая аналогия: Вместо скорости градиенты теперь будут задавать ускорение, но будет присутствовать сила трения.)\n",
    "\n",
    "```\n",
    "velocity = momentum * velocity - learning_rate * gradient \n",
    "w = w + velocity\n",
    "```\n",
    "\n",
    "`momentum` здесь коэффициент затухания, который тоже является гиперпараметром (к счастью, для него часто есть хорошее значение по умолчанию, типичный диапазон -- 0.8-0.99).\n",
    "\n",
    "Несколько полезных ссылок, где метод разбирается более подробно:  \n",
    "http://cs231n.github.io/neural-networks-3/#sgd  \n",
    "https://distill.pub/2017/momentum/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.323071, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.279769, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.297404, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.300147, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.274831, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.279916, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302557, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.286933, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.281800, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.336671, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.294825, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.270441, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.279329, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.269652, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.336324, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.336233, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.325737, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.298570, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.297682, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.300909, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement MomentumSGD.update function in optim.py\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=1e-4, learning_rate_decay=0.99)\n",
    "\n",
    "# You should see even better results than before!\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ну что, давайте уже тренировать сеть!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Последний тест - переобучимся (overfit) на маленьком наборе данных\n",
    "\n",
    "Хороший способ проверить, все ли реализовано корректно - переобучить сеть на маленьком наборе данных.  \n",
    "Наша модель обладает достаточной мощностью, чтобы приблизить маленький набор данных идеально, поэтому мы ожидаем, что на нем мы быстро дойдем до 100% точности на тренировочном наборе. \n",
    "\n",
    "Если этого не происходит, то где-то была допущена ошибка!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.331090, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.312129, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.320938, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.294606, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.281745, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.213331, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.224522, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.241363, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.133596, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.234422, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.995252, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.295022, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.057951, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.508024, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.267119, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.387697, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.611461, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.235971, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.598668, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.854929, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.030211, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.574025, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 2.021657, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 1.612543, Train accuracy: 0.400000, val accuracy: 0.066667\n",
      "Loss: 2.198184, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 2.004788, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.894248, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.870697, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 2.116166, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 1.470413, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Loss: 2.204922, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: 2.404071, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 2.416558, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 2.180847, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.842953, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.507100, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.806554, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.489478, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 2.036813, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.191979, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.308443, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.699236, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 2.255962, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.333877, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.703295, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.063860, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.326220, Train accuracy: 0.666667, val accuracy: 0.133333\n",
      "Loss: 1.649444, Train accuracy: 0.733333, val accuracy: 0.133333\n",
      "Loss: 2.220105, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.933758, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.385777, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.515105, Train accuracy: 0.733333, val accuracy: 0.133333\n",
      "Loss: 1.250673, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.591242, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.882332, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.567997, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.266077, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.613240, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.356840, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.545346, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.701882, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.647495, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.467913, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.068611, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.348356, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.491652, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.252142, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.372913, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.333402, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.671576, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 0.896147, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 0.974005, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.304631, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.140198, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.251783, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 0.920804, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.288114, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 0.914097, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.621333, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.871648, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 0.968531, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.356990, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.148390, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.327461, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.223619, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.320334, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.337950, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.298591, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.613899, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.331754, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.584494, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.905533, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.322617, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.009621, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.099114, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.297938, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.361139, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.293566, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.533882, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.311205, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.358078, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.183140, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.241988, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.310038, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 0.926917, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.256396, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.297815, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.479333, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.477309, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.076974, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.452676, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.154799, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 0.985925, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.058282, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.445400, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.304109, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.577202, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.511533, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.211309, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.228130, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.342992, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.500781, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.139017, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.393573, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.098830, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.166846, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.287057, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.265104, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.400445, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.418680, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.564257, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.378983, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.367496, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.267240, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.256026, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.288116, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.345122, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.206208, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.496008, Train accuracy: 1.000000, val accuracy: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.207602, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.175451, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.372289, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.229610, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.329939, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.357343, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.352635, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.280933, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.389550, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.383942, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.386335, Train accuracy: 1.000000, val accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "data_size = 15\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=150, batch_size=5)\n",
    "\n",
    "# You should expect this to reach 1.0 training accuracy \n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь найдем гипепараметры, для которых этот процесс сходится быстрее.\n",
    "Если все реализовано корректно, то существуют параметры, при которых процесс сходится в **20** эпох или еще быстрее.\n",
    "Найдите их!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.298954, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.301145, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.292137, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.251267, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.242947, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.277262, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.202803, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.074361, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.991511, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.013352, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.916159, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.641095, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.809735, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.679107, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.497139, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.114724, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 1.330389, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.607485, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Loss: 1.512382, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 1.604860, Train accuracy: 0.466667, val accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now, tweak some hyper parameters and make it train to 1.0 accuracy in 20 epochs or less\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-4)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "# TODO: Change any hyperparamers or optimizators to reach training accuracy in 20 epochs\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=20, batch_size=5)\n",
    "\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итак, основное мероприятие!\n",
    "\n",
    "Натренируйте лучшую нейросеть! Можно добавлять и изменять параметры, менять количество нейронов в слоях сети и как угодно экспериментировать. \n",
    "\n",
    "Добейтесь точности лучше **60%** на validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.872010, Train accuracy: 0.338667, val accuracy: 0.345000\n",
      "Loss: 1.587588, Train accuracy: 0.511000, val accuracy: 0.515000\n",
      "Loss: 2.012379, Train accuracy: 0.608111, val accuracy: 0.585000\n",
      "Loss: 1.023158, Train accuracy: 0.589222, val accuracy: 0.589000\n",
      "Loss: 1.904033, Train accuracy: 0.580111, val accuracy: 0.575000\n",
      "Loss: 1.407345, Train accuracy: 0.612444, val accuracy: 0.589000\n",
      "Loss: 1.423472, Train accuracy: 0.667333, val accuracy: 0.634000\n",
      "Loss: 1.321389, Train accuracy: 0.644778, val accuracy: 0.623000\n",
      "Loss: 1.797702, Train accuracy: 0.630333, val accuracy: 0.598000\n",
      "Loss: 1.326394, Train accuracy: 0.683556, val accuracy: 0.648000\n",
      "Loss: 2.225050, Train accuracy: 0.575444, val accuracy: 0.555000\n",
      "Loss: 1.655430, Train accuracy: 0.614000, val accuracy: 0.565000\n",
      "Loss: 1.496882, Train accuracy: 0.670889, val accuracy: 0.630000\n",
      "Loss: 1.400782, Train accuracy: 0.640333, val accuracy: 0.615000\n",
      "Loss: 1.463756, Train accuracy: 0.611556, val accuracy: 0.597000\n",
      "Loss: 1.616855, Train accuracy: 0.650444, val accuracy: 0.617000\n",
      "Loss: 2.045558, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 1.936061, Train accuracy: 0.361667, val accuracy: 0.373000\n",
      "Loss: 1.824032, Train accuracy: 0.490222, val accuracy: 0.496000\n",
      "Loss: 1.379842, Train accuracy: 0.604222, val accuracy: 0.589000\n",
      "Loss: 1.621486, Train accuracy: 0.661778, val accuracy: 0.643000\n",
      "Loss: 0.844585, Train accuracy: 0.682556, val accuracy: 0.644000\n",
      "Loss: 1.223682, Train accuracy: 0.703222, val accuracy: 0.688000\n",
      "Loss: 1.303939, Train accuracy: 0.697444, val accuracy: 0.671000\n",
      "Loss: 1.218581, Train accuracy: 0.700111, val accuracy: 0.668000\n",
      "Loss: 1.317929, Train accuracy: 0.690556, val accuracy: 0.659000\n",
      "Loss: 1.582971, Train accuracy: 0.705444, val accuracy: 0.657000\n",
      "Loss: 1.180835, Train accuracy: 0.694778, val accuracy: 0.628000\n",
      "Loss: 1.429637, Train accuracy: 0.686667, val accuracy: 0.634000\n",
      "Loss: 1.409493, Train accuracy: 0.730222, val accuracy: 0.686000\n",
      "Loss: 1.059760, Train accuracy: 0.748667, val accuracy: 0.697000\n",
      "Loss: 1.135647, Train accuracy: 0.765111, val accuracy: 0.698000\n",
      "Loss: 1.692530, Train accuracy: 0.400111, val accuracy: 0.374000\n",
      "Loss: 1.427567, Train accuracy: 0.546889, val accuracy: 0.534000\n",
      "Loss: 1.598039, Train accuracy: 0.574667, val accuracy: 0.581000\n",
      "Loss: 2.202219, Train accuracy: 0.558889, val accuracy: 0.549000\n",
      "Loss: 1.939057, Train accuracy: 0.566889, val accuracy: 0.561000\n",
      "Loss: 1.497462, Train accuracy: 0.600444, val accuracy: 0.600000\n",
      "Loss: 1.768990, Train accuracy: 0.616778, val accuracy: 0.563000\n",
      "Loss: 1.805682, Train accuracy: 0.643667, val accuracy: 0.603000\n",
      "Loss: 1.546490, Train accuracy: 0.622444, val accuracy: 0.594000\n",
      "Loss: 1.317267, Train accuracy: 0.627222, val accuracy: 0.575000\n",
      "Loss: 2.274898, Train accuracy: 0.597111, val accuracy: 0.575000\n",
      "Loss: 1.766661, Train accuracy: 0.670889, val accuracy: 0.630000\n",
      "Loss: 1.533451, Train accuracy: 0.632000, val accuracy: 0.611000\n",
      "Loss: 1.586547, Train accuracy: 0.635222, val accuracy: 0.617000\n",
      "Loss: 1.638331, Train accuracy: 0.576000, val accuracy: 0.560000\n",
      "Loss: 1.658601, Train accuracy: 0.658778, val accuracy: 0.628000\n",
      "Loss: 2.205001, Train accuracy: 0.202111, val accuracy: 0.209000\n",
      "Loss: 1.600752, Train accuracy: 0.411222, val accuracy: 0.416000\n",
      "Loss: 1.226191, Train accuracy: 0.565556, val accuracy: 0.564000\n",
      "Loss: 1.514898, Train accuracy: 0.614889, val accuracy: 0.613000\n",
      "Loss: 1.250332, Train accuracy: 0.680778, val accuracy: 0.660000\n",
      "Loss: 1.448754, Train accuracy: 0.619778, val accuracy: 0.604000\n",
      "Loss: 1.441596, Train accuracy: 0.666222, val accuracy: 0.630000\n",
      "Loss: 1.366991, Train accuracy: 0.683889, val accuracy: 0.627000\n",
      "Loss: 0.991117, Train accuracy: 0.631111, val accuracy: 0.588000\n",
      "Loss: 1.231697, Train accuracy: 0.716222, val accuracy: 0.662000\n",
      "Loss: 1.483535, Train accuracy: 0.720111, val accuracy: 0.662000\n",
      "Loss: 1.230090, Train accuracy: 0.707111, val accuracy: 0.676000\n",
      "Loss: 1.503031, Train accuracy: 0.757222, val accuracy: 0.694000\n",
      "Loss: 1.217997, Train accuracy: 0.726778, val accuracy: 0.670000\n",
      "Loss: 1.120650, Train accuracy: 0.757111, val accuracy: 0.693000\n",
      "Loss: 1.342959, Train accuracy: 0.748556, val accuracy: 0.696000\n",
      "Loss: 1.858308, Train accuracy: 0.361333, val accuracy: 0.343000\n",
      "Loss: 1.663665, Train accuracy: 0.528556, val accuracy: 0.506000\n",
      "Loss: 1.269235, Train accuracy: 0.555556, val accuracy: 0.544000\n",
      "Loss: 1.465777, Train accuracy: 0.605111, val accuracy: 0.559000\n",
      "Loss: 1.518073, Train accuracy: 0.605333, val accuracy: 0.561000\n",
      "Loss: 1.632043, Train accuracy: 0.675444, val accuracy: 0.632000\n",
      "Loss: 1.484537, Train accuracy: 0.644444, val accuracy: 0.614000\n",
      "Loss: 2.126772, Train accuracy: 0.546667, val accuracy: 0.519000\n",
      "Loss: 1.770770, Train accuracy: 0.673000, val accuracy: 0.617000\n",
      "Loss: 1.996135, Train accuracy: 0.630556, val accuracy: 0.591000\n",
      "Loss: 1.806058, Train accuracy: 0.586444, val accuracy: 0.549000\n",
      "Loss: 1.895812, Train accuracy: 0.630000, val accuracy: 0.611000\n",
      "Loss: 1.466692, Train accuracy: 0.682000, val accuracy: 0.627000\n",
      "Loss: 2.020755, Train accuracy: 0.657556, val accuracy: 0.630000\n",
      "Loss: 1.775695, Train accuracy: 0.652000, val accuracy: 0.623000\n",
      "Loss: 1.301369, Train accuracy: 0.618333, val accuracy: 0.584000\n",
      "Loss: 2.122538, Train accuracy: 0.213444, val accuracy: 0.221000\n",
      "Loss: 1.882806, Train accuracy: 0.450889, val accuracy: 0.462000\n",
      "Loss: 1.560077, Train accuracy: 0.547000, val accuracy: 0.545000\n",
      "Loss: 1.544036, Train accuracy: 0.607778, val accuracy: 0.577000\n",
      "Loss: 1.440154, Train accuracy: 0.645667, val accuracy: 0.610000\n",
      "Loss: 1.354069, Train accuracy: 0.681000, val accuracy: 0.632000\n",
      "Loss: 1.114077, Train accuracy: 0.696222, val accuracy: 0.659000\n",
      "Loss: 1.394419, Train accuracy: 0.729556, val accuracy: 0.676000\n",
      "Loss: 1.326257, Train accuracy: 0.743222, val accuracy: 0.682000\n",
      "Loss: 1.251686, Train accuracy: 0.728778, val accuracy: 0.677000\n",
      "Loss: 1.203037, Train accuracy: 0.731556, val accuracy: 0.682000\n",
      "Loss: 1.576845, Train accuracy: 0.743000, val accuracy: 0.684000\n",
      "Loss: 1.295844, Train accuracy: 0.743444, val accuracy: 0.669000\n",
      "Loss: 1.496532, Train accuracy: 0.714667, val accuracy: 0.661000\n",
      "Loss: 1.124240, Train accuracy: 0.726000, val accuracy: 0.673000\n",
      "Loss: 0.916391, Train accuracy: 0.749000, val accuracy: 0.680000\n",
      "Loss: 1.811104, Train accuracy: 0.361778, val accuracy: 0.354000\n",
      "Loss: 1.418812, Train accuracy: 0.542333, val accuracy: 0.522000\n",
      "Loss: 1.296260, Train accuracy: 0.581111, val accuracy: 0.573000\n",
      "Loss: 1.370776, Train accuracy: 0.642333, val accuracy: 0.598000\n",
      "Loss: 1.101128, Train accuracy: 0.639444, val accuracy: 0.611000\n",
      "Loss: 0.971290, Train accuracy: 0.671000, val accuracy: 0.621000\n",
      "Loss: 0.984959, Train accuracy: 0.697333, val accuracy: 0.646000\n",
      "Loss: 1.271778, Train accuracy: 0.668889, val accuracy: 0.631000\n",
      "Loss: 0.949020, Train accuracy: 0.705333, val accuracy: 0.641000\n",
      "Loss: 0.937289, Train accuracy: 0.700556, val accuracy: 0.639000\n",
      "Loss: 0.933268, Train accuracy: 0.722778, val accuracy: 0.655000\n",
      "Loss: 1.027765, Train accuracy: 0.756889, val accuracy: 0.680000\n",
      "Loss: 0.635917, Train accuracy: 0.718444, val accuracy: 0.665000\n",
      "Loss: 0.480160, Train accuracy: 0.768222, val accuracy: 0.677000\n",
      "Loss: 0.572353, Train accuracy: 0.765556, val accuracy: 0.688000\n",
      "Loss: 1.344433, Train accuracy: 0.771444, val accuracy: 0.685000\n",
      "Loss: 2.094005, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 1.875319, Train accuracy: 0.414778, val accuracy: 0.422000\n",
      "Loss: 1.277930, Train accuracy: 0.583889, val accuracy: 0.581000\n",
      "Loss: 1.199612, Train accuracy: 0.638889, val accuracy: 0.630000\n",
      "Loss: 0.910099, Train accuracy: 0.647556, val accuracy: 0.624000\n",
      "Loss: 1.029006, Train accuracy: 0.716222, val accuracy: 0.691000\n",
      "Loss: 0.603590, Train accuracy: 0.690889, val accuracy: 0.683000\n",
      "Loss: 0.980315, Train accuracy: 0.711333, val accuracy: 0.661000\n",
      "Loss: 0.865261, Train accuracy: 0.713333, val accuracy: 0.663000\n",
      "Loss: 0.728785, Train accuracy: 0.747889, val accuracy: 0.689000\n",
      "Loss: 0.712898, Train accuracy: 0.758778, val accuracy: 0.688000\n",
      "Loss: 0.937345, Train accuracy: 0.774667, val accuracy: 0.694000\n",
      "Loss: 0.681047, Train accuracy: 0.795333, val accuracy: 0.727000\n",
      "Loss: 0.622119, Train accuracy: 0.761222, val accuracy: 0.686000\n",
      "Loss: 0.750155, Train accuracy: 0.805222, val accuracy: 0.712000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.518339, Train accuracy: 0.764333, val accuracy: 0.692000\n",
      "Loss: 1.606728, Train accuracy: 0.374444, val accuracy: 0.394000\n",
      "Loss: 1.803521, Train accuracy: 0.502667, val accuracy: 0.489000\n",
      "Loss: 1.353962, Train accuracy: 0.598333, val accuracy: 0.598000\n",
      "Loss: 1.297465, Train accuracy: 0.552778, val accuracy: 0.540000\n",
      "Loss: 1.288564, Train accuracy: 0.674556, val accuracy: 0.624000\n",
      "Loss: 1.053110, Train accuracy: 0.668222, val accuracy: 0.622000\n",
      "Loss: 1.297445, Train accuracy: 0.708333, val accuracy: 0.675000\n",
      "Loss: 0.928541, Train accuracy: 0.717556, val accuracy: 0.673000\n",
      "Loss: 1.164817, Train accuracy: 0.687222, val accuracy: 0.643000\n",
      "Loss: 1.102808, Train accuracy: 0.748889, val accuracy: 0.678000\n",
      "Loss: 1.438138, Train accuracy: 0.733222, val accuracy: 0.666000\n",
      "Loss: 0.975801, Train accuracy: 0.729778, val accuracy: 0.653000\n",
      "Loss: 0.749295, Train accuracy: 0.743889, val accuracy: 0.673000\n",
      "Loss: 0.689419, Train accuracy: 0.751889, val accuracy: 0.685000\n",
      "Loss: 0.638491, Train accuracy: 0.778333, val accuracy: 0.682000\n",
      "Loss: 1.400592, Train accuracy: 0.768778, val accuracy: 0.673000\n",
      "Loss: 2.101945, Train accuracy: 0.237444, val accuracy: 0.238000\n",
      "Loss: 1.677684, Train accuracy: 0.402000, val accuracy: 0.396000\n",
      "Loss: 1.639670, Train accuracy: 0.578444, val accuracy: 0.587000\n",
      "Loss: 1.177888, Train accuracy: 0.617889, val accuracy: 0.601000\n",
      "Loss: 1.034620, Train accuracy: 0.675333, val accuracy: 0.639000\n",
      "Loss: 1.096867, Train accuracy: 0.655000, val accuracy: 0.638000\n",
      "Loss: 1.072844, Train accuracy: 0.727667, val accuracy: 0.678000\n",
      "Loss: 0.919606, Train accuracy: 0.762000, val accuracy: 0.694000\n",
      "Loss: 1.014584, Train accuracy: 0.768111, val accuracy: 0.693000\n",
      "Loss: 1.183345, Train accuracy: 0.777556, val accuracy: 0.712000\n",
      "Loss: 0.980724, Train accuracy: 0.759778, val accuracy: 0.696000\n",
      "Loss: 0.526650, Train accuracy: 0.760333, val accuracy: 0.684000\n",
      "Loss: 1.195331, Train accuracy: 0.791000, val accuracy: 0.717000\n",
      "Loss: 0.883251, Train accuracy: 0.794556, val accuracy: 0.713000\n",
      "Loss: 0.812381, Train accuracy: 0.794111, val accuracy: 0.708000\n",
      "Loss: 0.497980, Train accuracy: 0.832333, val accuracy: 0.727000\n",
      "Loss: 1.706924, Train accuracy: 0.376000, val accuracy: 0.401000\n",
      "Loss: 1.086800, Train accuracy: 0.533111, val accuracy: 0.535000\n",
      "Loss: 1.571126, Train accuracy: 0.629667, val accuracy: 0.601000\n",
      "Loss: 1.340643, Train accuracy: 0.619000, val accuracy: 0.566000\n",
      "Loss: 1.614522, Train accuracy: 0.681667, val accuracy: 0.622000\n",
      "Loss: 1.115037, Train accuracy: 0.656222, val accuracy: 0.607000\n",
      "Loss: 1.603915, Train accuracy: 0.698556, val accuracy: 0.636000\n",
      "Loss: 0.902076, Train accuracy: 0.724556, val accuracy: 0.679000\n",
      "Loss: 1.325390, Train accuracy: 0.709000, val accuracy: 0.657000\n",
      "Loss: 1.072737, Train accuracy: 0.730556, val accuracy: 0.658000\n",
      "Loss: 1.239260, Train accuracy: 0.734667, val accuracy: 0.653000\n",
      "Loss: 0.867480, Train accuracy: 0.730222, val accuracy: 0.632000\n",
      "Loss: 0.886557, Train accuracy: 0.756222, val accuracy: 0.661000\n",
      "Loss: 1.350500, Train accuracy: 0.781889, val accuracy: 0.695000\n",
      "Loss: 0.826541, Train accuracy: 0.764222, val accuracy: 0.661000\n",
      "Loss: 1.020902, Train accuracy: 0.772778, val accuracy: 0.665000\n",
      "Loss: 2.079185, Train accuracy: 0.232889, val accuracy: 0.233000\n",
      "Loss: 1.591073, Train accuracy: 0.482556, val accuracy: 0.481000\n",
      "Loss: 1.878380, Train accuracy: 0.580222, val accuracy: 0.576000\n",
      "Loss: 1.424213, Train accuracy: 0.626556, val accuracy: 0.586000\n",
      "Loss: 1.021016, Train accuracy: 0.668111, val accuracy: 0.650000\n",
      "Loss: 1.080581, Train accuracy: 0.718222, val accuracy: 0.668000\n",
      "Loss: 0.781515, Train accuracy: 0.711667, val accuracy: 0.666000\n",
      "Loss: 0.953229, Train accuracy: 0.745111, val accuracy: 0.676000\n",
      "Loss: 0.674935, Train accuracy: 0.747222, val accuracy: 0.667000\n",
      "Loss: 1.074869, Train accuracy: 0.763778, val accuracy: 0.685000\n",
      "Loss: 0.776174, Train accuracy: 0.782222, val accuracy: 0.701000\n",
      "Loss: 0.726749, Train accuracy: 0.820444, val accuracy: 0.705000\n",
      "Loss: 0.993355, Train accuracy: 0.826444, val accuracy: 0.718000\n",
      "Loss: 0.539646, Train accuracy: 0.801333, val accuracy: 0.696000\n",
      "Loss: 0.417646, Train accuracy: 0.814444, val accuracy: 0.709000\n",
      "Loss: 0.710281, Train accuracy: 0.821444, val accuracy: 0.703000\n",
      "Loss: 2.283187, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.138677, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.237753, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.221037, Train accuracy: 0.249222, val accuracy: 0.246000\n",
      "Loss: 2.081502, Train accuracy: 0.294889, val accuracy: 0.294000\n",
      "Loss: 1.948301, Train accuracy: 0.351333, val accuracy: 0.355000\n",
      "Loss: 1.581727, Train accuracy: 0.405667, val accuracy: 0.398000\n",
      "Loss: 1.720963, Train accuracy: 0.489667, val accuracy: 0.487000\n",
      "Loss: 1.624767, Train accuracy: 0.542333, val accuracy: 0.553000\n",
      "Loss: 1.426998, Train accuracy: 0.590556, val accuracy: 0.577000\n",
      "Loss: 1.362081, Train accuracy: 0.621556, val accuracy: 0.593000\n",
      "Loss: 1.327050, Train accuracy: 0.634000, val accuracy: 0.616000\n",
      "Loss: 1.109580, Train accuracy: 0.668111, val accuracy: 0.659000\n",
      "Loss: 1.237498, Train accuracy: 0.689778, val accuracy: 0.672000\n",
      "Loss: 1.278870, Train accuracy: 0.702222, val accuracy: 0.683000\n",
      "Loss: 0.963556, Train accuracy: 0.707111, val accuracy: 0.689000\n",
      "Loss: 2.198127, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.128337, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.149442, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.217476, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.210602, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.110518, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.249348, Train accuracy: 0.208444, val accuracy: 0.216000\n",
      "Loss: 2.118171, Train accuracy: 0.251333, val accuracy: 0.251000\n",
      "Loss: 2.063047, Train accuracy: 0.272222, val accuracy: 0.275000\n",
      "Loss: 2.093822, Train accuracy: 0.304222, val accuracy: 0.303000\n",
      "Loss: 1.873249, Train accuracy: 0.350444, val accuracy: 0.359000\n",
      "Loss: 1.703354, Train accuracy: 0.397333, val accuracy: 0.392000\n",
      "Loss: 1.931763, Train accuracy: 0.428222, val accuracy: 0.416000\n",
      "Loss: 1.621146, Train accuracy: 0.469222, val accuracy: 0.453000\n",
      "Loss: 1.571346, Train accuracy: 0.500889, val accuracy: 0.484000\n",
      "Loss: 1.705928, Train accuracy: 0.535000, val accuracy: 0.534000\n",
      "Loss: 2.184975, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.254744, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.128143, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.139714, Train accuracy: 0.267000, val accuracy: 0.263000\n",
      "Loss: 2.007202, Train accuracy: 0.310333, val accuracy: 0.322000\n",
      "Loss: 1.836670, Train accuracy: 0.416222, val accuracy: 0.419000\n",
      "Loss: 1.525765, Train accuracy: 0.474333, val accuracy: 0.467000\n",
      "Loss: 1.587233, Train accuracy: 0.542667, val accuracy: 0.532000\n",
      "Loss: 1.662034, Train accuracy: 0.591778, val accuracy: 0.580000\n",
      "Loss: 1.005003, Train accuracy: 0.634111, val accuracy: 0.612000\n",
      "Loss: 1.258927, Train accuracy: 0.655667, val accuracy: 0.632000\n",
      "Loss: 1.140477, Train accuracy: 0.671889, val accuracy: 0.643000\n",
      "Loss: 1.169260, Train accuracy: 0.695111, val accuracy: 0.671000\n",
      "Loss: 0.992830, Train accuracy: 0.709444, val accuracy: 0.674000\n",
      "Loss: 1.325819, Train accuracy: 0.720556, val accuracy: 0.687000\n",
      "Loss: 1.172132, Train accuracy: 0.728000, val accuracy: 0.686000\n",
      "Loss: 2.212069, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.218694, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.129398, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.172382, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.273855, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.128241, Train accuracy: 0.197000, val accuracy: 0.206000\n",
      "Loss: 2.155531, Train accuracy: 0.243667, val accuracy: 0.245000\n",
      "Loss: 2.121592, Train accuracy: 0.272333, val accuracy: 0.271000\n",
      "Loss: 2.050677, Train accuracy: 0.288111, val accuracy: 0.290000\n",
      "Loss: 2.055932, Train accuracy: 0.338222, val accuracy: 0.335000\n",
      "Loss: 1.667208, Train accuracy: 0.383778, val accuracy: 0.374000\n",
      "Loss: 1.698825, Train accuracy: 0.418889, val accuracy: 0.428000\n",
      "Loss: 1.581715, Train accuracy: 0.448444, val accuracy: 0.444000\n",
      "Loss: 1.750293, Train accuracy: 0.494667, val accuracy: 0.492000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.363953, Train accuracy: 0.520667, val accuracy: 0.518000\n",
      "Loss: 1.512634, Train accuracy: 0.559000, val accuracy: 0.550000\n",
      "Loss: 2.204760, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229846, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.096002, Train accuracy: 0.208444, val accuracy: 0.218000\n",
      "Loss: 2.105226, Train accuracy: 0.262667, val accuracy: 0.271000\n",
      "Loss: 1.913575, Train accuracy: 0.357667, val accuracy: 0.364000\n",
      "Loss: 1.739490, Train accuracy: 0.452556, val accuracy: 0.464000\n",
      "Loss: 1.738558, Train accuracy: 0.521667, val accuracy: 0.522000\n",
      "Loss: 1.347076, Train accuracy: 0.576556, val accuracy: 0.571000\n",
      "Loss: 1.304719, Train accuracy: 0.605333, val accuracy: 0.604000\n",
      "Loss: 1.292068, Train accuracy: 0.654111, val accuracy: 0.641000\n",
      "Loss: 1.252171, Train accuracy: 0.672111, val accuracy: 0.656000\n",
      "Loss: 1.019288, Train accuracy: 0.688222, val accuracy: 0.670000\n",
      "Loss: 1.134058, Train accuracy: 0.708778, val accuracy: 0.682000\n",
      "Loss: 0.988271, Train accuracy: 0.723333, val accuracy: 0.706000\n",
      "Loss: 1.014050, Train accuracy: 0.718444, val accuracy: 0.694000\n",
      "Loss: 1.302888, Train accuracy: 0.744778, val accuracy: 0.723000\n",
      "Loss: 2.197552, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.267674, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.283066, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.240403, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.033326, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.097908, Train accuracy: 0.211333, val accuracy: 0.219000\n",
      "Loss: 2.223276, Train accuracy: 0.256444, val accuracy: 0.254000\n",
      "Loss: 2.097259, Train accuracy: 0.279333, val accuracy: 0.280000\n",
      "Loss: 1.828140, Train accuracy: 0.308333, val accuracy: 0.310000\n",
      "Loss: 1.896040, Train accuracy: 0.374333, val accuracy: 0.369000\n",
      "Loss: 1.720822, Train accuracy: 0.406333, val accuracy: 0.396000\n",
      "Loss: 1.471231, Train accuracy: 0.459444, val accuracy: 0.460000\n",
      "Loss: 1.563769, Train accuracy: 0.491556, val accuracy: 0.476000\n",
      "Loss: 1.647943, Train accuracy: 0.529778, val accuracy: 0.524000\n",
      "Loss: 1.355731, Train accuracy: 0.552889, val accuracy: 0.554000\n",
      "Loss: 1.388458, Train accuracy: 0.582778, val accuracy: 0.559000\n",
      "Loss: 2.221109, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.234221, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.208660, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.168440, Train accuracy: 0.270889, val accuracy: 0.268000\n",
      "Loss: 1.838605, Train accuracy: 0.303889, val accuracy: 0.310000\n",
      "Loss: 1.918975, Train accuracy: 0.392778, val accuracy: 0.396000\n",
      "Loss: 1.465015, Train accuracy: 0.459556, val accuracy: 0.451000\n",
      "Loss: 1.318759, Train accuracy: 0.520444, val accuracy: 0.516000\n",
      "Loss: 1.275258, Train accuracy: 0.579444, val accuracy: 0.567000\n",
      "Loss: 1.336319, Train accuracy: 0.614333, val accuracy: 0.613000\n",
      "Loss: 0.928692, Train accuracy: 0.649556, val accuracy: 0.645000\n",
      "Loss: 1.052097, Train accuracy: 0.675778, val accuracy: 0.655000\n",
      "Loss: 0.868656, Train accuracy: 0.672000, val accuracy: 0.654000\n",
      "Loss: 1.139307, Train accuracy: 0.709778, val accuracy: 0.679000\n",
      "Loss: 1.267047, Train accuracy: 0.710667, val accuracy: 0.689000\n",
      "Loss: 0.586028, Train accuracy: 0.719667, val accuracy: 0.685000\n",
      "Loss: 2.322097, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.287709, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.361116, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.263991, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.219862, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.397291, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.143150, Train accuracy: 0.222111, val accuracy: 0.228000\n",
      "Loss: 1.869084, Train accuracy: 0.260778, val accuracy: 0.260000\n",
      "Loss: 1.936964, Train accuracy: 0.279333, val accuracy: 0.276000\n",
      "Loss: 2.098301, Train accuracy: 0.297000, val accuracy: 0.300000\n",
      "Loss: 2.011036, Train accuracy: 0.352556, val accuracy: 0.354000\n",
      "Loss: 1.844246, Train accuracy: 0.404889, val accuracy: 0.393000\n",
      "Loss: 1.778943, Train accuracy: 0.449778, val accuracy: 0.430000\n",
      "Loss: 1.651133, Train accuracy: 0.483000, val accuracy: 0.458000\n",
      "Loss: 1.519002, Train accuracy: 0.510778, val accuracy: 0.504000\n",
      "Loss: 1.753233, Train accuracy: 0.551222, val accuracy: 0.536000\n",
      "Loss: 2.313858, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.322060, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.130930, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.259359, Train accuracy: 0.251778, val accuracy: 0.246000\n",
      "Loss: 2.085146, Train accuracy: 0.310889, val accuracy: 0.318000\n",
      "Loss: 1.731533, Train accuracy: 0.403111, val accuracy: 0.394000\n",
      "Loss: 1.335637, Train accuracy: 0.496778, val accuracy: 0.493000\n",
      "Loss: 1.804016, Train accuracy: 0.549000, val accuracy: 0.530000\n",
      "Loss: 1.306896, Train accuracy: 0.598778, val accuracy: 0.580000\n",
      "Loss: 1.456877, Train accuracy: 0.641778, val accuracy: 0.615000\n",
      "Loss: 1.309288, Train accuracy: 0.671222, val accuracy: 0.644000\n",
      "Loss: 1.193881, Train accuracy: 0.683778, val accuracy: 0.660000\n",
      "Loss: 1.053148, Train accuracy: 0.703333, val accuracy: 0.683000\n",
      "Loss: 1.097783, Train accuracy: 0.709778, val accuracy: 0.682000\n",
      "Loss: 0.915866, Train accuracy: 0.724778, val accuracy: 0.697000\n",
      "Loss: 0.595836, Train accuracy: 0.729333, val accuracy: 0.701000\n",
      "Loss: 2.213225, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.265672, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.246693, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.276894, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.206147, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.172727, Train accuracy: 0.196778, val accuracy: 0.206000\n",
      "Loss: 2.058461, Train accuracy: 0.242778, val accuracy: 0.241000\n",
      "Loss: 1.829872, Train accuracy: 0.283444, val accuracy: 0.288000\n",
      "Loss: 2.116359, Train accuracy: 0.317222, val accuracy: 0.319000\n",
      "Loss: 1.724728, Train accuracy: 0.353333, val accuracy: 0.353000\n",
      "Loss: 1.883266, Train accuracy: 0.395556, val accuracy: 0.387000\n",
      "Loss: 1.551814, Train accuracy: 0.451444, val accuracy: 0.448000\n",
      "Loss: 1.521759, Train accuracy: 0.501333, val accuracy: 0.498000\n",
      "Loss: 1.564668, Train accuracy: 0.539889, val accuracy: 0.540000\n",
      "Loss: 1.317951, Train accuracy: 0.561667, val accuracy: 0.540000\n",
      "Loss: 1.399748, Train accuracy: 0.579444, val accuracy: 0.573000\n",
      "Loss: 2.089457, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.177726, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.294709, Train accuracy: 0.210111, val accuracy: 0.218000\n",
      "Loss: 1.928784, Train accuracy: 0.273889, val accuracy: 0.272000\n",
      "Loss: 1.663933, Train accuracy: 0.370556, val accuracy: 0.372000\n",
      "Loss: 1.562599, Train accuracy: 0.447111, val accuracy: 0.441000\n",
      "Loss: 1.614892, Train accuracy: 0.517222, val accuracy: 0.518000\n",
      "Loss: 1.499559, Train accuracy: 0.584778, val accuracy: 0.576000\n",
      "Loss: 1.330171, Train accuracy: 0.613667, val accuracy: 0.605000\n",
      "Loss: 1.269482, Train accuracy: 0.653222, val accuracy: 0.642000\n",
      "Loss: 1.079861, Train accuracy: 0.681667, val accuracy: 0.669000\n",
      "Loss: 0.860237, Train accuracy: 0.688111, val accuracy: 0.653000\n",
      "Loss: 1.186071, Train accuracy: 0.711556, val accuracy: 0.683000\n",
      "Loss: 0.789214, Train accuracy: 0.720333, val accuracy: 0.689000\n",
      "Loss: 1.010388, Train accuracy: 0.735222, val accuracy: 0.715000\n",
      "Loss: 1.193050, Train accuracy: 0.749667, val accuracy: 0.708000\n",
      "Loss: 2.217502, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.231816, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.327550, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.208737, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.161343, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.176573, Train accuracy: 0.216444, val accuracy: 0.222000\n",
      "Loss: 2.001407, Train accuracy: 0.264222, val accuracy: 0.261000\n",
      "Loss: 2.134932, Train accuracy: 0.283000, val accuracy: 0.288000\n",
      "Loss: 1.773773, Train accuracy: 0.332222, val accuracy: 0.339000\n",
      "Loss: 1.839995, Train accuracy: 0.389000, val accuracy: 0.393000\n",
      "Loss: 1.677530, Train accuracy: 0.431556, val accuracy: 0.430000\n",
      "Loss: 1.559273, Train accuracy: 0.470000, val accuracy: 0.452000\n",
      "Loss: 1.357529, Train accuracy: 0.514778, val accuracy: 0.509000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.444856, Train accuracy: 0.545000, val accuracy: 0.537000\n",
      "Loss: 1.357962, Train accuracy: 0.570444, val accuracy: 0.554000\n",
      "Loss: 1.047661, Train accuracy: 0.592778, val accuracy: 0.580000\n",
      "best validation accuracy achieved: 0.727000\n"
     ]
    }
   ],
   "source": [
    "# Let's train the best one-hidden-layer network we can\n",
    "\n",
    "learning_rates = [1e-1, 1e-2]\n",
    "reg_strength = [1e-3, 1e-5]\n",
    "hidden_layer_sizes = [64, 128, 256]\n",
    "batch_size = [64, 128]\n",
    "num_epochs = 16\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = 0\n",
    "\n",
    "loss_history = []\n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "\n",
    "# TODO find the best hyperparameters to train the network\n",
    "# Don't hesitate to add new values to the arrays above, perform experiments, use any tricks you want\n",
    "# You should expect to get to at least 40% of valudation accuracy\n",
    "# Save loss/train/history of the best classifier to the variables above\n",
    "for lr in learning_rates:\n",
    "    for rs in reg_strength:\n",
    "        for ls in hidden_layer_sizes:\n",
    "            for bs in batch_size:\n",
    "                model = TwoLayerNet(n_input=train_X.shape[1], n_output=10, hidden_layer_size=ls, reg=rs)\n",
    "                trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=lr, num_epochs=num_epochs, batch_size=bs)\n",
    "                temp_loss_history, temp_train_history, temp_val_history = trainer.fit()\n",
    "                \n",
    "                if temp_val_history[-1] > best_val_accuracy:\n",
    "                    best_classifier = model\n",
    "                    best_val_accuracy = temp_val_history[-1]\n",
    "                    loss_history = temp_loss_history.copy()\n",
    "                    train_history = temp_train_history.copy()\n",
    "                    val_history = temp_val_history.copy()\n",
    "    \n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f1f55447ee0>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(211)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(loss_history)\n",
    "plt.subplot(212)\n",
    "plt.title(\"Train/validation accuracy\")\n",
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как обычно, посмотрим, как наша лучшая модель работает на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural net test set accuracy: 0.679000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Neural net test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
